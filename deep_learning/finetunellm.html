<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine Tuning LLM</title>
    <style>
        body {
            font-family: 'Courier New', monospace;
            line-height: 1.6;
            background-image: url('../assets/images/wooden.jpg');
            background-size: cover;
            background-attachment: fixed;
            background-repeat: no-repeat;
            background-position: center;
            margin: 0;
            padding: 20px;
        }

        .container {
            font-family: 'Courier New', monospace;
            max-width: 1200px;
            margin: auto;
            background: rgba(161, 151, 151, 0.9);
            padding: 20px;
            border-radius: 8px;
            border-color: #ddd;
            box-shadow: 0 0 10px rgb(252, 252, 252);
        }

        h1,
        h2,
        h3 {
            color: #333;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        table,
        th,
        td {
            border: 1px solid #ddd;
        }

        th,
        td {
            padding: 10px;
            text-align: left;
        }

        th {
            background-color: #292727;
            color: white;
            text-align: center;
        }

        code {
            /* background-color: #f4f4f4; */
            padding: 2px 4px;
            border-radius: 4px;
        }

        .floating-link {
            position: fixed;
            /* Keeps it floating */
            bottom: 20px;
            /* 20px from the bottom */
            right: 20px;
            /* 20px from the right */
            background-color: #007BFF;
            /* Button color */
            color: white;
            /* Text color */
            padding: 10px 15px;
            border-radius: 50px;
            text-decoration: none;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            font-family: 'Courier New', monospace;
            transition: background-color 0.3s ease;
        }

        /* Hover effect */
        .floating-link:hover {
            background-color: #0056b3;
        }

        .floating-sidebar {
            position: fixed;
            /* Keeps it floating */
            top: 50%;
            /* Vertically centered */
            left: 0;
            /* Stick to the left */
            transform: translateY(-50%);
            /* Perfect center */
            background-color: rgba(0, 0, 0, 0.5);
            /* Semi-transparent background */
            padding: 15px;
            border-radius: 0 10px 10px 0;
            /* Rounded right corners */
            box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.3);
        }

        /* Sidebar Link */
        .floating-sidebar a {
            color: white;
            text-decoration: none;
            font-size: 18px;
            font-family: 'Courier New', monospace;
            display: block;
            padding: 10px 0;
            transition: color 0.3s ease;
        }

        /* Hover effect */
        .floating-sidebar a:hover {
            color: #FFD700;
            /* Golden color on hover */
        }
    </style>
</head>

<body>
    <div class="container">
        <div>
            <h2>üöÄ Training a Large Language Model (LLM)</h2>
            <p>The training of an LLM involves two major stages: <strong>Pre-Training</strong> and
                <strong>Fine-Tuning</strong>. This process helps transform a general-purpose language model into a
                specialized tool tailored for specific tasks or domains.
            </p>

            <h3>üîπ 1. Pre-Training Stage (Base Model)</h3>
            <p><strong>üìå What It Is:</strong><br>
                The LLM is trained on vast datasets sourced from the <em>Internet, books, articles</em>, etc., using
                <strong>FP32 precision</strong>. This helps the model build a broad understanding of language patterns,
                grammar, facts, and reasoning.
            </p>

            <p><strong>üõ†Ô∏è Examples of Pre-Trained Models:</strong><br>
                GPT-4 Turbo, GPT-3.5 ‚Äî trained on massive datasets to become versatile general-purpose models.</p>

            <p><strong>‚ö° Key Characteristics:</strong><br>
                - Unsupervised learning on diverse data.<br>
                - Foundation for all downstream fine-tuning.</p>

            <h3>üîπ 2. Fine-Tuning Stage</h3>
            <p>After pre-training, LLMs undergo fine-tuning to specialize them for <strong>specific tasks</strong> or
                <strong>domains</strong>. There are various types of fine-tuning, each optimized for different goals and
                resource constraints.
            </p>

            <div id="Types-of-Fine-Tuning-Methods">
                <h2>Types of Fine-Tuning Methods</h2>

                <h3>1. Supervised Fine-Tuning</h3>
                <p>Supervised fine-tuning involves further training a pre-trained model using a task-specific dataset
                    with labeled input-output pairs. This process allows the model to learn how to map inputs to outputs
                    based on the given dataset.</p>
                <h4>Process:</h4>
                <ul>
                    <li>Use a pre-trained model.</li>
                    <li>Prepare a dataset with input-output pairs as expected by the model.</li>
                    <li>Adjust the pre-trained weights during fine-tuning to adapt the model to the new task.</li>
                </ul>
                <p>Supervised fine-tuning is ideal for tasks such as sentiment analysis, text classification, and named
                    entity recognition where labeled datasets are available.</p>

                <h3>2. Instruction Fine-Tuning</h3>
                <p>Instruction fine-tuning augments input-output examples with detailed instructions in the prompt
                    template. This allows the model to generalize better to new tasks, especially those involving
                    natural language instructions.</p>
                <h4>Process:</h4>
                <ul>
                    <li>Use a pre-trained model.</li>
                    <li>Prepare a dataset in the form of instruction-response pairs.</li>
                    <li>Train the model with the instruction fine-tuning process, similar to neural network training.
                    </li>
                </ul>
                <p>Instruction fine-tuning is commonly used in building chatbots, question answering systems, and other
                    tasks that require natural language interaction.</p>

                <h3>3. Parameter-Efficient Fine-Tuning (PEFT)</h3>
                <p>Training a full model is resource-intensive. PEFT methods enable the efficient use of memory and
                    computation by modifying only a subset of the model's parameters, significantly reducing the
                    required memory for training.</p>

                <h4>PEFT Methods:</h4>
                <ul>
                    <li><strong>Selective Method:</strong> Freeze most layers of the model and only fine-tune specific
                        layers.</li>
                    <li><strong>Reparameterization Method (LoRA):</strong> Use low-rank matrices to reparameterize model
                        weights, freezing the original weights and adding small, trainable parameters.</li>
                    <li><strong>Additive Method:</strong> Add new layers to the encoder or decoder side of the model and
                        train these for the specific task.</li>
                    <li><strong>Soft Prompting:</strong> Train only the new tokens added to the model prompt, keeping
                        other tokens and weights frozen.</li>
                </ul>
                <p>PEFT is useful when working with large models that exceed memory limits, reducing both training costs
                    and resource requirements.</p>

                <h3>4. Reinforcement Learning with Human Feedback (RLHF)</h3>
                <p>RLHF aligns a fine-tuned model's output to human preferences using reinforcement learning. This
                    method refines model behavior after the initial fine-tuning phase.</p>
                <h4>Process:</h4>
                <ul>
                    <li><strong>Prepare Dataset:</strong> Generate prompt-completion pairs and rank them based on human
                        evaluators' alignment criteria.</li>
                    <li><strong>Train Reward Model:</strong> Build a reward model that scores completions based on human
                        feedback.</li>
                    <li><strong>Update Model:</strong> Use reinforcement learning, typically the PPO algorithm, to
                        update the model weights based on the reward model.</li>
                </ul>
                <p>RLHF is ideal for tasks where human-like outputs are necessary, such as generating text that aligns
                    with user expectations or ethical guidelines.</p>

                <h2>Prompt Engineering vs RAG vs Fine-Tuning</h2>
                <p>Let us explore the difference between prompt engineering, RAG, and fine-tuning.</p>

                <table border="1">
                    <tr>
                        <th>Criteria</th>
                        <th>Prompt Engineering</th>
                        <th>RAG</th>
                        <th>Fine-Tuning</th>
                    </tr>
                    <tr>
                        <td><strong>Purpose</strong></td>
                        <td>Focuses on writing an effective prompt to optimize output.</td>
                        <td>Retrieves relevant information for a given prompt from an external database.</td>
                        <td>Trains and adapts a model for a specific task.</td>
                    </tr>
                    <tr>
                        <td><strong>Model</strong></td>
                        <td>Model weights are not updated.</td>
                        <td>Model weights are not updated.</td>
                        <td>Model weights are updated.</td>
                    </tr>
                    <tr>
                        <td><strong>Complexity</strong></td>
                        <td>No technical knowledge required.</td>
                        <td>Less complex compared to fine-tuning; requires skills in vector databases and retrieval
                            mechanisms.</td>
                        <td>Requires technical knowledge.</td>
                    </tr>
                    <tr>
                        <td><strong>Compute Cost</strong></td>
                        <td>Very low; only costs related to API calls.</td>
                        <td>Cost-effective compared to fine-tuning.</td>
                        <td>May require specialized hardware depending on model and dataset size.</td>
                    </tr>
                    <tr>
                        <td><strong>Knowledge</strong></td>
                        <td>The model does not learn new data.</td>
                        <td>The prompt is equipped with new data as context.</td>
                        <td>The model learns new data.</td>
                    </tr>
                </table>

                <h2>Benefits of Fine-Tuning LLMs</h2>
                <ul>
                    <li><strong>Increased Performance:</strong> Fine-tuned models adapt to new data, leading to more
                        accurate and reliable outputs.</li>
                    <li><strong>Efficiency:</strong> Fine-tuning saves computational costs by adapting pre-trained
                        models rather than training a model from scratch.</li>
                    <li><strong>Domain Adaptation:</strong> LLMs can be tailored to specific industries like medical,
                        legal, or financial domains by focusing on relevant terminology and structures.</li>
                    <li><strong>Better Generalization:</strong> Models fine-tuned on task-specific data generalize
                        better to the unique patterns and structures of the task.</li>
                </ul>
            </div>

        </div>
    </div><br>
    </div>
    <br>
    <div class="container">
        <div>
            <h2>üîó LoRA (Low-Rank Adaptation)</h2>
            <p><strong>LoRA</strong> is a parameter-efficient fine-tuning (PEFT) technique designed to adapt large
                pre-trained language models (LLMs) without the need to update all model parameters. It introduces
                <strong>low-rank matrices</strong> into specific layers (usually attention layers) during training,
                significantly reducing computational overhead and memory usage.
            </p>

            <h3>üõ† How LoRA Works:</h3>
            <ul>
                <li><strong>Decomposition:</strong> Instead of fine-tuning full weight matrices (<strong>W</strong>),
                    LoRA decomposes them into two smaller matrices (<strong>A</strong> and <strong>B</strong>) with a
                    much lower rank (<strong>r</strong>):<br>
                    <code>W' = W + ŒîW = W + A * B</code><br>
                    where A ‚àà ‚Ñù<sup>(d √ó r)</sup> and B ‚àà ‚Ñù<sup>(r √ó k)</sup>, with r ‚â™ min(d, k).
                </li>
                <li><strong>Frozen Base Model:</strong> The original model weights (<strong>W</strong>) remain frozen.
                    Only the low-rank matrices (<strong>A</strong> and <strong>B</strong>) are trained.</li>
                <li><strong>Benefits:</strong>
                    <ul>
                        <li>Drastically reduces trainable parameters.</li>
                        <li>Lowers GPU memory usage.</li>
                        <li>Enables fine-tuning large models on consumer hardware.</li>
                    </ul>
                </li>
            </ul>

            <h2>‚ö° QLoRA (Quantized LoRA)</h2>
            <p><strong>QLoRA</strong> extends LoRA by introducing <strong>quantization</strong> during fine-tuning,
                making it even more memory-efficient and suitable for extremely large models (e.g., 65B+ parameters) on
                limited hardware (single GPU setups).</p>

            <h3>üõ† Key Components of QLoRA:</h3>
            <ul>
                <li><strong>4-bit Quantization:</strong> Base model weights are quantized to <strong>4-bit
                        precision</strong> (e.g., using NF4 quantization), reducing memory usage while preserving
                    performance.</li>
                <li><strong>LoRA Adapters on Quantized Models:</strong> Low-rank matrices (<strong>A</strong>,
                    <strong>B</strong>) are introduced into the attention layers of the <strong>4-bit quantized
                        model</strong>, with adapters trained in higher precision (e.g., FP16/FP32).
                </li>
                <li><strong>Double Quantization (Optional):</strong> Further compresses quantization statistics to
                    reduce memory overhead.</li>
                <li><strong>Paged Optimizers:</strong> Optimizers like AdamW are adapted to handle quantized tensors
                    efficiently.</li>
            </ul>

            <h3>‚öñÔ∏è LoRA vs. QLoRA:</h3>
            <table border="1" cellpadding="5">
                <tr>
                    <th>Aspect</th>
                    <th>LoRA</th>
                    <th>QLoRA</th>
                </tr>
                <tr>
                    <td><strong>Base Model Precision</strong></td>
                    <td>FP16/FP32</td>
                    <td>4-bit Quantized (NF4)</td>
                </tr>
                <tr>
                    <td><strong>Trainable Params</strong></td>
                    <td>Low-rank matrices (A, B)</td>
                    <td>Low-rank matrices (A, B)</td>
                </tr>
                <tr>
                    <td><strong>Memory Usage</strong></td>
                    <td>Moderate reduction</td>
                    <td>Significant reduction</td>
                </tr>
                <tr>
                    <td><strong>Hardware Needs</strong></td>
                    <td>Multi-GPU setups for large LLMs</td>
                    <td>Single GPU fine-tuning (e.g., 48GB VRAM)</td>
                </tr>
                <tr>
                    <td><strong>Performance</strong></td>
                    <td>High fidelity</td>
                    <td>Near-LoRA performance with higher efficiency</td>
                </tr>
            </table>

            <h3>üöÄ Use Cases for LoRA/QLoRA:</h3>
            <ul>
                <li>Fine-tuning <strong>LLMs</strong> (e.g., LLaMA, GPT) for downstream tasks.</li>
                <li>Domain-specific adaptations (e.g., medical, legal corpora).</li>
                <li>Resource-constrained environments (e.g., consumer GPUs).</li>
                <li>Rapid prototyping and experimentation on large-scale models.</li>
            </ul>
        </div>
        <br>
        <div class="container">
            <div>
                <h2>üìê Mathematical Explanation of LoRA (Low-Rank Adaptation)</h2>
                <p>The core idea of LoRA is to approximate a <strong>full-rank weight update</strong> (ŒîW) using a
                    <strong>low-rank decomposition</strong>.
                </p>

                <h3>üî∏ 1. Problem Setup</h3>
                <p>Suppose we have a pre-trained weight matrix W‚ÇÄ ‚àà ‚Ñù<sup>d √ó k</sup> used in a <strong>linear
                        layer</strong>:</p>
                <pre>y = W‚ÇÄ x</pre>
                <p>During standard fine-tuning, W‚ÇÄ is updated to W = W‚ÇÄ + ŒîW. LoRA approximates ŒîW as:</p>
                <pre>ŒîW ‚âà B A</pre>
                <ul>
                    <li><strong>A</strong> ‚àà ‚Ñù<sup>r √ó k</sup></li>
                    <li><strong>B</strong> ‚àà ‚Ñù<sup>d √ó r</sup></li>
                    <li><strong>r</strong> ‚â™ min(d, k) is the <em>rank</em></li>
                </ul>

                <h3>üî∏ 2. Example Walkthrough</h3>
                <h4>Example Setup:</h4>
                <p>Let‚Äôs consider:</p>
                <ul>
                    <li><strong>W‚ÇÄ</strong> ‚àà ‚Ñù<sup>4 √ó 4</sup>:</li>
                </ul>
                <pre>
          W‚ÇÄ = [[1, 2, 3, 4],
                [2, 3, 4, 5],
                [3, 4, 5, 6],
                [4, 5, 6, 7]]
            </pre>
                <p>We aim to approximate ŒîW with rank <strong>r = 2</strong>.</p>

                <h4>Step 1: Initialize Low-Rank Matrices</h4>
                <p>Define matrices A and B:</p>
                <pre>
          A = [[1, 0, 1, 0],
               [0, 1, 0, 1]]
          
          B = [[1, 0],
               [0, 1],
               [1, 0],
               [0, 1]]
            </pre>

                <h4>Step 2: Compute Low-Rank Approximation</h4>
                <p>Compute ŒîW = B √ó A:</p>
                <pre>
          ŒîW = [[1, 0, 1, 0],
                [0, 1, 0, 1],
                [1, 0, 1, 0],
                [0, 1, 0, 1]]
            </pre>

                <h4>Step 3: Apply Weight Update</h4>
                <p>Update the weight matrix:</p>
                <pre>
          W = W‚ÇÄ + ŒîW
          
          = [[2, 2, 4, 4],
             [2, 4, 4, 6],
             [4, 4, 6, 6],
             [4, 6, 6, 8]]
            </pre>

                <h3>üî∏ 3. Parameter Efficiency</h3>
                <ul>
                    <li><strong>Full-Rank Update:</strong> 4 √ó 4 = 16 parameters.</li>
                    <li><strong>LoRA Update:</strong> A (2 √ó 4) + B (4 √ó 2) = 8 + 8 = 16 parameters (in this toy
                        example).
                    </li>
                </ul>
                <p>In real scenarios (e.g., d = k = 1024, r = 8):</p>
                <ul>
                    <li><strong>Full-Rank:</strong> 1,048,576 parameters.</li>
                    <li><strong>LoRA:</strong> 16,384 parameters (~98.4% reduction).</li>
                </ul>

                <h3>üî∏ 4. Why Does This Work?</h3>
                <ul>
                    <li>Weight updates often lie in a <strong>low-dimensional manifold</strong>.</li>
                    <li>Low-rank decomposition <strong>regularizes</strong> the update and prevents overfitting.</li>
                    <li>LoRA captures the <strong>essential transformations</strong> with fewer parameters.</li>
                </ul>
            </div>

        </div><br>
        <div class="container">
            <div>
                <h2>üßÆ QLoRA (Quantized Low-Rank Adaptation) Explained</h2>
                <p><strong>QLoRA</strong> combines <em>quantization</em> and <em>Low-Rank Adaptation (LoRA)</em> to
                    fine-tune large models efficiently while using minimal GPU memory.</p>

                <h3>üî∏ 1. Core Idea</h3>
                <p>QLoRA enhances LoRA by applying <strong>quantization</strong> to the base model weights, reducing
                    memory usage while preserving fine-tuning performance through low-rank updates.</p>

                <ul>
                    <li><strong>Quantization:</strong> Compresses pre-trained model weights (e.g., from FP16 to 4-bit or
                        8-bit).</li>
                    <li><strong>LoRA Layers:</strong> Add low-rank adapters (matrices A and B) to fine-tune the model.
                    </li>
                </ul>

                <h3>üî∏ 2. Mathematical Explanation</h3>
                <p>For a linear layer with weight matrix W‚ÇÄ ‚àà ‚Ñù<sup>d √ó k</sup>:</p>
                <pre>y = W‚ÇÄ x</pre>

                <h4>Step 1: Quantize W‚ÇÄ</h4>
                <p>Quantize W‚ÇÄ into Q(W‚ÇÄ) using 4-bit or 8-bit quantization:</p>
                <pre>Q(W‚ÇÄ) ‚âà round(W‚ÇÄ / s) + z</pre>
                <ul>
                    <li><strong>s:</strong> scale factor</li>
                    <li><strong>z:</strong> zero-point for asymmetric quantization</li>
                </ul>

                <h4>Step 2: Apply Low-Rank Adaptation (LoRA)</h4>
                <p>Approximate the weight update ŒîW as:</p>
                <pre>ŒîW ‚âà B A</pre>
                <ul>
                    <li><strong>A</strong> ‚àà ‚Ñù<sup>r √ó k</sup></li>
                    <li><strong>B</strong> ‚àà ‚Ñù<sup>d √ó r</sup></li>
                    <li><strong>r</strong> ‚â™ min(d, k) is the low rank</li>
                </ul>

                <h4>Final Adapted Output:</h4>
                <pre>y = Q(W‚ÇÄ) x + Œ± (B A) x</pre>
                <ul>
                    <li><strong>Œ±:</strong> LoRA scaling factor</li>
                </ul>

                <h3>üî∏ 3. Example Walkthrough</h3>

                <h4>Example Setup:</h4>
                <ul>
                    <li><strong>W‚ÇÄ</strong> ‚àà ‚Ñù<sup>4 √ó 4</sup>:</li>
                </ul>
                <pre>
          W‚ÇÄ = [[1.1, 2.3, 3.5, 4.7],
                [2.2, 3.3, 4.4, 5.5],
                [3.3, 4.4, 5.5, 6.6],
                [4.4, 5.5, 6.6, 7.7]]
            </pre>

                <h4>Step 1: Quantization (4-bit)</h4>
                <p>Using min-max scaling, quantize W‚ÇÄ to 4-bit:</p>
                <pre>
          Q(W‚ÇÄ) ‚âà [[1, 2, 4, 5],
                    [2, 3, 4, 6],
                    [3, 4, 5, 7],
                    [4, 5, 6, 7]]
            </pre>

                <h4>Step 2: Low-Rank Decomposition</h4>
                <p>Choose rank r = 2, and define:</p>
                <pre>
          A = [[1, 0, 1, 0],
               [0, 1, 0, 1]]
          
          B = [[1, 0],
               [0, 1],
               [1, 0],
               [0, 1]]
            </pre>

                <p>Compute ŒîW = B √ó A:</p>
                <pre>
          ŒîW = [[1, 0, 1, 0],
                [0, 1, 0, 1],
                [1, 0, 1, 0],
                [0, 1, 0, 1]]
            </pre>

                <h4>Step 3: Final Adapted Weights</h4>
                <p>Apply the low-rank update with scaling factor Œ± = 0.1:</p>
                <pre>
          W_final = Q(W‚ÇÄ) + Œ± ŒîW
                  ‚âà [[1.1, 2.0, 4.1, 5.0],
                      [2.0, 3.1, 4.0, 6.1],
                      [3.1, 4.0, 5.1, 7.0],
                      [4.0, 5.1, 6.0, 7.1]]
            </pre>

                <h3>üî∏ 4. Benefits of QLoRA</h3>
                <ul>
                    <li>üíæ <strong>Memory-Efficient:</strong> Quantizing the base model reduces GPU memory usage
                        significantly.</li>
                    <li>‚ö° <strong>Compute-Efficient:</strong> Fine-tuning only low-rank matrices A and B.</li>
                    <li>üéØ <strong>High Accuracy:</strong> Retains performance close to full-precision fine-tuning.</li>
                    <li>üöÄ <strong>Scalability:</strong> Enables fine-tuning large LLMs (e.g., 65B) on consumer
                        hardware.</li>
                </ul>
            </div>

        </div>
        <div class="container">
            <div>
                <h2>üßÆ Low-Rank Adaptation (LoRA) Explained</h2>
                <p><strong>LoRA</strong> fine-tunes large models efficiently by approximating weight updates with
                    low-rank matrices.</p>

                <h3>üî∏ 1. Core Concept</h3>
                <p>Instead of updating the full weight matrix <code>W</code>, LoRA decomposes it into two smaller
                    trainable matrices:</p>
                <pre>W = W‚ÇÄ + ŒîW = W‚ÇÄ + B A</pre>

                <ul>
                    <li><strong>W‚ÇÄ</strong>: Frozen pre-trained weights</li>
                    <li><strong>A</strong> ‚àà ‚Ñù<sup>r √ó k</sup>, <strong>B</strong> ‚àà ‚Ñù<sup>d √ó r</sup>: Trainable
                        low-rank matrices</li>
                    <li><strong>r</strong> ‚â™ min(d, k): Low-rank constraint</li>
                </ul>

                <h3>üî∏ 2. Mathematical Explanation</h3>
                <p>For an input <strong>x</strong>, the forward pass is modified as:</p>
                <pre>h = W‚ÇÄ x + (B A) x</pre>

                <ul>
                    <li><code>W‚ÇÄ x</code>: Frozen pre-trained computation</li>
                    <li><code>(B A) x</code>: Trainable low-rank adaptation</li>
                </ul>

                <h3>üî∏ 3. Example Walkthrough</h3>

                <h4>Example Setup:</h4>
                <ul>
                    <li><strong>W‚ÇÄ</strong> ‚àà ‚Ñù<sup>3 √ó 2</sup>:</li>
                </ul>
                <pre>
        W‚ÇÄ = [[1, 2],
              [3, 4],
              [5, 6]]
                </pre>

                <h4>Step 1: Initialize LoRA Matrices</h4>
                <p>Choose rank r = 1 and define:</p>
                <pre>
        A = [0.1, -0.2]
        B = [0, 0, 0]  # Initially zero
                </pre>

                <h4>Step 2: After Training</h4>
                <pre>
        B (updated) = [0.5, 1.0, 1.5]
        A (updated) = [0.3, -0.1]
                </pre>

                <p>Compute ŒîW:</p>
                <pre>
        ŒîW = B A = [
          [0.15, -0.05],
          [0.3, -0.1],
          [0.45, -0.15]
        ]
                </pre>

                <h4>Step 3: Final Adapted Weights</h4>
                <p>Apply the LoRA update:</p>
                <pre>
        W_final = W‚ÇÄ + ŒîW = [
          [1.15, 1.95],
          [3.3, 3.9],
          [5.45, 5.85]
        ]
                </pre>

                <h3>üî∏ 4. Benefits of LoRA</h3>
                <ul>
                    <li>üíæ <strong>Memory-Efficient:</strong> Reduces trainable parameters by ~10,000√ó.</li>
                    <li>‚ö° <strong>Zero Inference Latency:</strong> Merged into base weights after training.</li>
                    <li>üéØ <strong>Performance:</strong> Matches full fine-tuning while using fewer resources.</li>
                </ul>
            </div>
        </div>

</body>

</html>