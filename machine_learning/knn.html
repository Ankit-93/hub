<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding K-Nearest Neighbors (KNN)</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

    <style>
        /* Global Styles */
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 0;
            color: #ffffff;
            background: url('../assets/images/ml-Background.jpg') no-repeat center center fixed;
            background-size: cover;
            position: relative;
        }

        /* Dark Overlay */
        body::before {
            content: "";
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1;
        }

        /* Layout Containers */
        header,
        footer {
            background: linear-gradient(135deg, #0829df, #04013c);
            text-align: center;
            padding: 20px;
            font-size: 24px;
            position: relative;
            z-index: 10;
        }

        main {
            max-width: 900px;
            margin: 40px auto;
            padding: 25px;
            background: rgba(19, 50, 185, 0.85);
            border-radius: 10px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 10;
        }

        h1,
        h2,
        h3 {
            color: #d9dcdf;
        }

        .case h3 {
            color: #ffcc00;
        }

        .case p {
            color: #ffffff;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: rgba(255, 255, 255, 0.1);
        }

        th,
        td {
            border: 1px solid rgba(255, 255, 255, 0.43);
            padding: 12px;
            text-align: center;
            color: #ffffff;
        }

        th {
            background: rgba(255, 255, 255, 0.2);
            font-weight: bold;
        }

        .image-container {
            text-align: center;
            margin: 5px 0;
        }

        .image-container img {
            width: 85%;
            border-radius: 5px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
        }

        /* Keep Flexbox for content before "Conclusion" */
        .content-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 10px;
        }

        .image-container {
            flex: 1;
        }

        .image-container img {
            width: 100%;
            border-radius: 10px;
        }

        .text-container {
            flex: 1;
        }

        /* Remove Flexbox after "Conclusion" */
        .failure-cases {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            display: block;
            /* Stack elements instead of flex */
            text-align: justify;
        }

        /* Individual Case Boxes */
        .case {
            width: 100%;
            background: rgba(255, 255, 255, 0.15);
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 6px;
        }

        .case h3 {
            color: #ffcc00;
            margin-bottom: 5px;
        }

        .case p {
            color: #ffffff;
        }
    </style>
</head>
<header>
    <h1>K-Nearest Neighbors (KNN)</h1>
</header>

<body>
    <div class="failure-cases">
        <div class="case">
            <h1>Understanding K-Nearest Neighbors (KNN)</h1>
            <p>K-Nearest Neighbors (KNN) is a simple,
                yet powerful,
                supervised learning algorithm used for classification and regression. It makes predictions based on the
                majority
                class of its nearest neighbors. </p>
            <h2>Steps to Calculate KNN</h2>
            <h3>1. Choose the Number of Neighbors (K)</h3>
            <p>The first step is to select the number of neighbors (<strong>K</strong>). A small K value (e.g., K=3)
                makes the
                model more sensitive to noise,
                while a larger K value provides smoother decision boundaries. </p>
            <h3>2. Compute the Distance</h3>
            <p>For a given query point,
                calculate the distance between it and all other points in the dataset. The most common distance metric
                is the
                <strong>Euclidean distance</strong>,
                which is computed as:
            </p>
            <div class="highlight"><strong>Euclidean Distance Formula:</strong><br>d=√((x₂ - x₁)² + (y₂ - y₁)²) </div>
            <h3>3. Identify the K Nearest Neighbors</h3>
            <p>Sort all the distances in ascending order and select the top K closest points to the query point. </p>
            <h3>4. Determine the Majority Class</h3>
            <p>Among the K nearest neighbors,
                count the occurrences of each class label. The class with the highest count is assigned to the query
                point. </p>
            <h3>5. Assign the Class to the Query Point</h3>
            <p>The final step is to classify the query point based on the majority class of its nearest neighbors. </p>
            <h2>Example: Toy Dataset</h2>
            <p>Let's consider a small dataset where we have points classified into two categories (0 and 1).</p>

            <table>
                <tr>
                    <th>X</th>
                    <th>Y</th>
                    <th>Label</th>
                </tr>
                <tr>
                    <td>1</td>
                    <td>2</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>3</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>3</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>8</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td>8</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>7</td>
                    <td>1</td>
                </tr>
            </table>
            <p>Now,
                if we have a query point at (5, 5) and choose K=3,
                we calculate distances:</p>
            <table>
                <tr>
                    <th>Point (X, Y)</th>
                    <th>Distance to (5, 5)</th>
                    <th>Label</th>
                </tr>
                <tr>
                    <td>(1, 2)</td>
                    <td>5.00</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>(2, 3)</td>
                    <td>3.61</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>(3, 3)</td>
                    <td>2.83</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>(6, 8)</td>
                    <td>3.16</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(7, 8)</td>
                    <td>3.61</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>(8, 7)</td>
                    <td>3.61</td>
                    <td>1</td>
                </tr>
            </table>
            <div class="content-container">
                <div class="image-container"><img src="../assets/images/knn.png" alt="K-Nearest Neighbors"></div>
                <div class="text-container">
                    <p>The 3 nearest neighbors are:</p>
                    <ul>
                        <li>(3, 3) - Class 0</li>
                        <li>(6, 8) - Class 1</li>
                        <li>(2, 3) - Class 0</li>
                    </ul>
                    <p>Since Class 0 appears twice and Class 1 appears once,
                        the query point (5, 5) is classified as <strong>Class 0</strong>.</p>
                    <h2>Key Characteristics of KNN</h2>
                    <ul>
                        <li>KNN is a <strong>non-parametric</strong>algorithm,
                            meaning it makes no assumptions about the data distribution.</li>
                        <li>It is a <strong>lazy learner</strong>,
                            meaning it stores the training data and makes predictions at runtime.</li>
                        <li>It works well with small datasets but can become slow for large datasets due to distance
                            calculations.</li>
                    </ul>
                    <h2>Conclusion</h2>
                    <p>KNN is an intuitive and effective algorithm for classification tasks. However,
                        choosing an optimal K value and handling large datasets efficiently are key challenges in using
                        KNN.
                    </p>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <h2>Failure Cases of KNN</h2>
        <div class="case">
            <h3>1. High Computational Cost</h3>
            <p>KNN stores all training data and computes distances at runtime,
                making it slow for large datasets.</p>
            <h3>2. Curse of Dimensionality</h3>
            <p>As the number of features increases,
                distance calculations become less meaningful,
                reducing accuracy.</p>
            <h3>3. Sensitive to Noisy Data</h3>
            <p>KNN is easily affected by outliers and mislabeled data,
                which can lead to incorrect classifications.</p>
            <h3>4. Imbalanced Data Problem</h3>
            <p>If one class is much larger than the others,
                KNN may be biased toward the majority class.</p>
            <h3>5. Choosing the Wrong K Value</h3>
            <p>Too small a K value makes KNN sensitive to noise,
                while too large a K value may cause misclassification.</p>
        </div>
    </div>
    <div class="failure-cases">
        <h2>Distance Measures in Machine Learning</h2>
        <div class="case">

            <div>
                <h3>1. Euclidean Distance (L2 Norm)</h3>
                <p>It represents the straight-line distance between two points.</p>
                <p><strong>Formula:</strong> d(A, B) = √(∑ (Aᵢ - Bᵢ)²)</p>
                <p><strong>Example:</strong> Given two points A(2, 3) and B(5, 7):</p>
                <p>d(A, B) = √((5 - 2)² + (7 - 3)²) = √(9 + 16) = √25 = 5</p>
            </div>

            <div>
                <h3>2. Manhattan Distance (L1 Norm, City Block Distance)</h3>
                <p>Measures the sum of absolute differences between coordinates.</p>
                <p><strong>Formula:</strong> d(A, B) = ∑ |Aᵢ - Bᵢ|</p>
                <p><strong>Example:</strong> Given two points A(2, 3) and B(5, 7):</p>
                <p>d(A, B) = |5 - 2| + |7 - 3| = 3 + 4 = 7</p>
            </div>

            <div>
                <h3>3. Minkowski Distance</h3>
                <p>A generalized distance metric.</p>
                <p><strong>Formula:</strong> d(A, B) = (∑ |Aᵢ - Bᵢ|ᵖ)^(1/p)</p>
                <p><strong>Example:</strong> With p=3, given A(2, 3) and B(5, 7):</p>
                <p>d(A, B) = (|5 - 2|³ + |7 - 3|³)^(1/3) = (27 + 64)^(1/3) ≈ 4.64</p>
            </div>

            <div>
                <h3>4. Hamming Distance</h3>
                <p>Counts differing positions in binary or categorical data.</p>
                <p><strong>Formula:</strong> d(A, B) = ∑ 𝟙(Aᵢ ≠ Bᵢ)</p>
                <p><strong>Example:</strong> Comparing A = "1011101" and B = "1001001":</p>
                <p>d(A, B) = Differences at 3rd, 5th, and 6th positions → Hamming Distance = 3</p>
            </div>

            <div>
                <h3>5. L1 Norm (Taxicab Norm)</h3>
                <p>Another name for Manhattan Distance.</p>
                <p><strong>Formula:</strong> ||A||₁ = ∑ |Aᵢ|</p>
                <p><strong>Example:</strong> A vector A = (3, -4, 5):</p>
                <p>||A||₁ = |3| + |-4| + |5| = 3 + 4 + 5 = 12</p>
            </div>

            <div>
                <h3>6. L2 Norm (Euclidean Norm)</h3>
                <p>Another name for Euclidean Distance.</p>
                <p><strong>Formula:</strong> ||A||₂ = √(∑ Aᵢ²)</p>
                <p><strong>Example:</strong> A vector A = (3, -4, 5):</p>
                <p>||A||₂ = √(3² + (-4)² + 5²) = √(9 + 16 + 25) = √50 ≈ 7.07</p>
            </div>

            <div>
                <h3>Summary Table:</h3>
                <table border="1">
                    <tr>
                        <th>Distance Metric</th>
                        <th>Formula</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td>Euclidean (L2)</td>
                        <td>√(∑ (Aᵢ - Bᵢ)²)</td>
                        <td>d((2,3), (5,7)) = 5</td>
                    </tr>
                    <tr>
                        <td>Manhattan (L1)</td>
                        <td>∑ |Aᵢ - Bᵢ|</td>
                        <td>d((2,3), (5,7)) = 7</td>
                    </tr>
                    <tr>
                        <td>Minkowski (p=3)</td>
                        <td>(∑ |Aᵢ - Bᵢ|³)^(1/3)</td>
                        <td>d((2,3), (5,7)) ≈ 4.64</td>
                    </tr>
                    <tr>
                        <td>Hamming</td>
                        <td>∑ 𝟙(Aᵢ ≠ Bᵢ)</td>
                        <td>d("1011101", "1001001") = 3</td>
                    </tr>
                    <tr>
                        <td>L1 Norm</td>
                        <td>∑ |Aᵢ|</td>
                        <td>|| (3, -4, 5) ||₁ = 12</td>
                    </tr>
                    <tr>
                        <td>L2 Norm</td>
                        <td>√(∑ Aᵢ²)</td>
                        <td>|| (3, -4, 5) ||₂ ≈ 7.07</td>
                    </tr>
                </table>
            </div>
        </div>

    </div>

    <div class="failure-cases">
        <div>
            <h2>Cosine Similarity & Cosine Distance</h2>
        </div>
        <div class="case">

            <div>
                <h3>1. Cosine Similarity</h3>
                <p>Cosine similarity measures the cosine of the angle between two vectors.</p>
                <p><strong>Formula:</strong></p>
                <p>Cosine Similarity = \( \cos(\theta) = \frac{A \cdot B}{||A||_2 ||B||_2} \)</p>
                <p><strong>Example:</strong></p>
                <p>For vectors A(3,4) and B(4,3):</p>
                <p>Dot product: \( 3 \times 4 + 4 \times 3 = 24 \)</p>
                <p>Magnitude of A: \( \sqrt{3^2 + 4^2} = 5 \)</p>
                <p>Magnitude of B: \( \sqrt{4^2 + 3^2} = 5 \)</p>
                <p>Cosine Similarity: \( \frac{24}{5 \times 5} = 0.96 \) (Highly similar)</p>
            </div>

            <div>
                <h3>2. Cosine Distance</h3>
                <p>Cosine distance is derived from cosine similarity:</p>
                <p><strong>Formula:</strong></p>
                <p>Cosine Distance = \( 1 - \cos(\theta) \)</p>
                <p>For our example: \( 1 - 0.96 = 0.04 \) (Very small distance)</p>
            </div>

            <div>
                <h3>3. Relationship with Euclidean Distance</h3>
                <p>For normalized vectors, Euclidean distance and cosine similarity are related:</p>
                <p>\( d_{\text{Euclidean}}^2 = 2 (1 - \cos(\theta)) \)</p>
            </div>

            <div>
                <h3>4. Visual Representation</h3>
                <p>The diagram below illustrates the difference between Euclidean Distance and Cosine Similarity.</p>
                <img src="../assets/images/similarity.png" alt="Cosine Similarity vs. Euclidean Distance Diagram"
                    width="1000">
            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div>
            <h2>How Good KNN is ?</h2>
        </div>
        <div class="case">
            <div>
                <h2>Performance of KNN for Amazon Fine Food Reviews (364K Data Points)</h2>

                <div>
                    <h3>1. Time Complexity of KNN</h3>
                    <p><strong>Training Time:</strong> \( O(1) \) (No model training required)</p>
                    <p><strong>Prediction Time:</strong></p>
                    <ul>
                        <li>Brute Force Search: \( O(N \times d) \) per query</li>
                        <li>Using KD-Tree / Ball-Tree (for low dimensions): \( O(\log N) \)</li>
                        <li>Using Approximate Nearest Neighbors (FAISS, Annoy): \( O(k \log N) \)</li>
                    </ul>
                    <p><strong>Issue:</strong> For \( N = 364K \), brute-force KNN is very slow.</p>
                </div>

                <div>
                    <h3>2. Memory Complexity of KNN</h3>
                    <p>Memory usage depends on the number of features \( d \).</p>
                    <ul>
                        <li>TF-IDF (Sparse Matrix, 364K × 10K): ~3-5 GB RAM</li>
                        <li>Word2Vec (Dense Matrix, 364K × 300): ~400 MB RAM</li>
                        <li>BERT Embeddings (364K × 768): ~2.2 GB RAM</li>
                    </ul>
                    <p><strong>Issue:</strong> KNN stores the entire dataset in memory, making it inefficient for large
                        datasets.</p>
                </div>

                <div>
                    <h3>3. Performance Bottlenecks</h3>
                    <ul>
                        <li>Predicting one review takes minutes using brute-force KNN.</li>
                        <li>High RAM usage due to storing all 364K reviews.</li>
                    </ul>
                </div>

                <div>
                    <h3>4. Optimizations for KNN on Large Datasets</h3>
                    <ul>
                        <li>✅ <strong>Use Approximate Nearest Neighbors (ANN)</strong>
                            <ul>
                                <li><strong>FAISS</strong> (Facebook AI Similarity Search) - Fast and scalable.</li>
                                <li><strong>Annoy</strong> - Optimized for large datasets.</li>
                                <li><strong>HNSW</strong> - Graph-based nearest neighbors search.</li>
                            </ul>
                        </li>
                        <li>✅ <strong>Dimensionality Reduction</strong>
                            <ul>
                                <li>PCA (Reduces 300 features to ~50).</li>
                                <li>t-SNE / UMAP for visualization.</li>
                            </ul>
                        </li>
                        <li>✅ <strong>Use Faster Algorithms</strong>
                            <ul>
                                <li>Naïve Bayes (Fast for text classification).</li>
                                <li>SVM, Logistic Regression (More scalable).</li>
                                <li>Transformer Models (BERT, RoBERTa - Best accuracy).</li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <div>
                    <h3>5. Conclusion: Should You Use KNN for 364K Reviews?</h3>
                    <ul>
                        <li>❌ Not recommended for real-time applications.</li>
                        <li>❌ High memory usage makes it inefficient.</li>
                        <li>✅ Works if you use optimizations (FAISS, dimensionality reduction).</li>
                        <li>✅ Use KNN only if interpretability is crucial.</li>
                    </ul>
                    <p><strong>Alternative:</strong> Consider SVM, Naïve Bayes, or Transformer-based models for
                        better performance.</p>
                </div>

            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div>
            <h2>Decision Surface in K-Nearest Neighbors (KNN)</h2>
        </div>
        <div class="case">

            <div>
                <h3>1. What is a Decision Surface?</h3>
                <p>A <strong>decision surface</strong> (or boundary) is the dividing line that separates different
                    classes in a classification problem.
                    In <strong>KNN</strong>, the shape of the decision surface depends on the number of neighbors
                    (<strong>K</strong>).</p>
            </div>

            <div>
                <h3>2. How the Decision Surface Changes with K?</h3>
                <div class="content-container">
                    <div class="image-container">
                        <img src="../assets/images/casestudy1.webp" alt="K-Nearest Neighbors">
                    </div>

                    <div class="text-container">
                        <h4>🔹 K = 1 (Overfitting, Very Detailed Boundaries)</h4>
                        <p>- The model follows each individual data point closely.</p>
                        <p>- The boundary is highly irregular and sensitive to noise.</p>

                        <h4>🔹 K = 3 (Balanced, Smooth but Still Detailed)</h4>
                        <p>- The boundary is still detailed but less sensitive to small variations.</p>
                        <p>- Less overfitting than K=1.</p>

                        <h4>🔹 K = 5 (Smooth, More Generalized)</h4>
                        <p>- The boundary becomes smoother.</p>
                        <p>- The model generalizes better but loses some fine details.</p>

                        <h4>🔹 K = 10 (Underfitting, Too Simple)</h4>
                        <p>- The boundary is very smooth and almost linear.</p>
                        <p>- The model underfits the data and loses key patterns.</p>

                    </div>
                </div>
            </div>
            <div>
                <h3>3. Summary Table: Impact of K on Decision Boundary</h3>
                <table border="1" cellpadding="5">
                    <tr>
                        <th>K Value</th>
                        <th>Decision Boundary Shape</th>
                        <th>Effect</th>
                    </tr>
                    <tr>
                        <td><strong>K = 1</strong></td>
                        <td>Highly irregular, jagged</td>
                        <td>Overfits, memorizes training data</td>
                    </tr>
                    <tr>
                        <td><strong>K = 3</strong></td>
                        <td>Balanced, smooth but detailed</td>
                        <td>Good tradeoff between bias and variance</td>
                    </tr>
                    <tr>
                        <td><strong>K = 5</strong></td>
                        <td>Smooth and generalized</td>
                        <td>Less overfitting, better generalization</td>
                    </tr>
                    <tr>
                        <td><strong>K = 10</strong></td>
                        <td>Very smooth, almost linear</td>
                        <td>Underfits, loses details</td>
                    </tr>
                </table>
            </div>

            <div>
                <h3>4. Choosing the Best K?</h3>
                <p>- Use <strong>cross-validation</strong> to find the best K.</p>
                <p>- A common rule of thumb: <strong>K = sqrt(N)</strong> (square root of the dataset size).</p>
                <p>- If <strong>data is noisy</strong>, use a higher K to smooth out fluctuations.</p>
            </div>

            <div>
                <h3>5. Conclusion</h3>
                <p>📌 <strong>Small K (1, 3, 5)</strong> → Complex, detailed boundaries, but may overfit.</p>
                <p>📌 <strong>Large K (20, 50, 100)</strong> → Smooth, generalized boundaries, but may underfit.</p>
                <p>📌 <strong>Best K?</strong> → Found using <strong>cross-validation</strong>.</p>
            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div>
            <h2>Understanding Overfitting and Underfitting</h2>
            <div class="case">
                <h3>📌 What is Underfitting?</h3>
                <div class="content-container">
                    <div class="image-container">
                        <img src="../assets/images/underfit.webp" alt="Underfitting Example">
                        <p><em>Figure: Underfitting - Model is too simple to capture patterns.</em></p>
                    </div>
                    <div class="container">
                        <div class="text">
                            <p>Underfitting happens when the model is too simple to learn patterns from data.</p>
                            <p>It performs poorly on both training and test data.</p>
                            <p>Example: Using K=100 in KNN creates a decision boundary that is too smooth, missing
                                key
                                details.</p>
                            <p><strong>Analogy:</strong> A student who doesn’t study and guesses answers.</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="case">
                <h3>📌 What is Overfitting?</h3>
                <div class="content-container">
                    <div class="image-container">
                        <img src="../assets/images/overfit.webp" alt="Overfitting Example">
                        <p><em>Figure: Overfitting - Model memorizes noise and outliers.</em></p>
                    </div>
                    <div class="container"></div>
                    <div class="text">
                        <p>Overfitting happens when the model is too complex and memorizes data instead of
                            generalizing.</p>
                        <p>It performs very well on training data but fails on test data.</p>
                        <p>Example: Using K=1 in KNN, the model follows every noise in data.</p>
                        <p><strong>Analogy:</strong> A student who memorizes answers but fails new questions.</p>
                    </div>
                </div>
            </div>

            <div class="case">
                <h3>✅ The Ideal Model (Good Fit)</h3>
                <div class="content-container">
                    <div class="image-container">
                        <img src="../assets/images/balanced-fit.webp" alt="Good Fit Example">
                        <p><em>Figure: Overfitting - Model memorizes noise and outliers.</em></p>
                    </div>
                    <!-- GOOD FIT SECTION -->
                    <div class="container">
                        <div class="text">
                            <p>The best model balances complexity and generalization.</p>
                            <p>It performs well on both training and test data without overfitting or underfitting.</p>
                            <p>Example: Choosing K=5 or K=10 in KNN gives a smooth but useful decision boundary.</p>
                            <p><strong>Analogy:</strong> A student who understands concepts instead of memorizing.</p>
                        </div>
                    </div>

                </div>

                <div class="failure-cases">

                    <h2>What is the Train Time Complexity of KNN & KD-Tree?</h2>
                    <div class="case">
                        <p>
                        <h3><strong>For KNN (K-Nearest Neighbors):</strong><br></h3>
                        - Training space complexity: O(nd)<br>
                        - Training time complexity: O(nd)<br>
                        - Testing/Run time complexity: O(n * k * d), where:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;- <b>n</b> is the number of data points<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;- <b>k</b> is the number of nearest neighbors considered<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;- <b>d</b> is the number of dimensions/features
                        </p>
                        <div>
                            <h3>What are the assumptions of KNN ?</h3>
                            <p>The KNN algorithm assumes that similar things exist in close proximity. In other words,
                                similar things are near to each other.
                            <ul>
                                <li>KNN’s main disadvantage of becoming significantly slower as the volume of data
                                    increases
                                    makes it an impractical choice in environments
                                    where predictions need to be made rapidly.</li>
                                <li>Moreover, there are faster algorithms that can
                                    produce more accurate classification and
                                    regression results.</li>
                            </ul>
                            </p>
                        </div>
                        <div>
                            <h3>What happens if we do not normalize our dataset before classification using KNN ?</h3>
                            <p>The k-nearest neighbor algorithm relies on majority voting based on class membership of
                                'k' nearest samples for a given test point.
                                The nearness of samples is typically based on Euclidean distance.
                                Feature scaling refers to the methods used to normalize the range of values of
                                independent variables. In other words, the ways to set the
                                feature value range within a similar scale.
                                Feature magnitude matters for several reasons:
                                <li>The scale of the variable directly influences the classification coefficient.</li>
                                <li>Variables with a more significant magnitude dominate over the ones with a smaller
                                    magnitude range.</li>
                                <li>Euclidean distances are sensitive to feature magnitude.</li>
                                To overcome this effect, all features have to be at the same level of scale , especially
                                for distance based algorithms
                            </p>
                        </div>
                        <div>
                            <h3>How is Weighted KNN Algorithm Better than Simple KNN Algorithm?</h3>
                            <p>
                                <strong>Issue with KNN:</strong> simplest method is to take the majority vote, but this
                                can be a problem if the nearest neighbors vary widely in their
                                distance and the closest neighbors more reliably indicate the class of the object.
                            <div class="failure-cases">
                                <div class="case">
                                    <h3>1. Weighted KNN:</h3>
                                    <p>Weighted KNN assigns weights to the neighbors based on their distance to the
                                        query point. The weight can be inversely proportional to the distance, so closer
                                        neighbors have more influence on the prediction.</p>
                                    <h3>2. Advantages of Weighted KNN:</h3>
                                    <ul>
                                        <li>🔹 More accurate predictions by giving more weight to closer neighbors.</li>
                                        <li>🔹 Reduces the impact of outliers and noisy data points.</li>
                                        <li>🔹 Improves the performance of KNN in cases where the nearest neighbors are
                                            not equally reliable.</li>
                                    </ul>
                                    <h3>3. Weighted KNN Formula:</h3>
                                    <p>The weighted prediction is calculated as:</p>
                                    <div class="highlight">Weighted Prediction = \( \frac{\sum_{i=1}^{k} w_i \times
                                        y_i}{\sum_{i=1}^{k} w_i} \)</div>
                                    <p>Where:</p>
                                    <ul>
                                        <li> \( w_i \) is the weight assigned to the ith neighbor based on its distance.
                                        </li>
                                        <li> \( y_i \) is the class label of the ith neighbor.</li>
                                        <li> \( k \) is the number of neighbors considered.</li>
                                    </ul>
                                    <h3>4. Conclusion:</h3>
                                    <p>Weighted KNN is a more sophisticated version of the KNN algorithm that improves
                                        prediction accuracy by considering the reliability of each neighbor based on its
                                        distance to the query point.</p>
                                </div>
                            </div>
                        </div>


                    </div>
                    <div class="failure-cases">
                        <div class="case">
                            <div>
                                <h2>k in k-NN in Terms of Bias</h2>

                                <h3>
                                    Which of the following will be true about k in k-NN in terms of Bias?
                                </h3>
                                <ul>
                                    <li><b>A)</b> When you increase the k, the bias will increase ✅</li>
                                    <li><b>B)</b> When you decrease the k, the bias will increase ❌</li>
                                    <li><b>C)</b> Can’t say ❌</li>
                                    <li><b>D)</b> None of these ❌</li>
                                </ul>
                                <p><b>Solution:</b> A) A large K means a simpler model, and a simpler model is always
                                    considered to have high bias.</p>
                                <h3>
                                    Which of the following will be true about k in k-NN in terms of Variance?
                                </h3>
                                <ul>
                                    <li><b>A)</b> When you increase the k, the variance will increase ❌</li>
                                    <li><b>B)</b> When you decrease the k, the variance will increase ✅</li>
                                    <li><b>C)</b> Can’t say ❌</li>
                                    <li><b>D)</b> None of these ❌</li>
                                </ul>
                                <p><b>Solution:</b> B) A simple model is considered a low variance model, meaning that
                                    when k decreases, variance increases.</p>
                            </div>

                        </div>
                    </div>

</body>

</html>