<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine Tuning LLM</title>
    <style>
        body {
            font-family: 'Courier New', monospace;
            line-height: 1.6;
            background-image: url('../assets/images/wooden.jpg');
            background-size: cover;
            background-attachment: fixed;
            background-repeat: no-repeat;
            background-position: center;
            margin: 0;
            padding: 20px;
        }

        .container {
            font-family: 'Courier New', monospace;
            max-width: 1200px;
            margin: auto;
            background: rgba(161, 151, 151, 0.9);
            padding: 20px;
            border-radius: 8px;
            border-color: #ddd;
            box-shadow: 0 0 10px rgb(252, 252, 252);
        }

        h1,
        h2,
        h3 {
            color: #333;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        table,
        th,
        td {
            border: 1px solid #ddd;
        }

        th,
        td {
            padding: 10px;
            text-align: left;
        }

        th {
            background-color: #292727;
            color: white;
            text-align: center;
        }

        code {
            /* background-color: #f4f4f4; */
            padding: 2px 4px;
            border-radius: 4px;
        }

        .floating-link {
            position: fixed;
            /* Keeps it floating */
            bottom: 20px;
            /* 20px from the bottom */
            right: 20px;
            /* 20px from the right */
            background-color: #007BFF;
            /* Button color */
            color: white;
            /* Text color */
            padding: 10px 15px;
            border-radius: 50px;
            text-decoration: none;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            font-family: 'Courier New', monospace;
            transition: background-color 0.3s ease;
        }

        /* Hover effect */
        .floating-link:hover {
            background-color: #0056b3;
        }

        .floating-sidebar {
            position: fixed;
            /* Keeps it floating */
            top: 50%;
            /* Vertically centered */
            left: 0;
            /* Stick to the left */
            transform: translateY(-50%);
            /* Perfect center */
            background-color: rgba(0, 0, 0, 0.5);
            /* Semi-transparent background */
            padding: 15px;
            border-radius: 0 10px 10px 0;
            /* Rounded right corners */
            box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.3);
        }

        /* Sidebar Link */
        .floating-sidebar a {
            color: white;
            text-decoration: none;
            font-size: 18px;
            font-family: 'Courier New', monospace;
            display: block;
            padding: 10px 0;
            transition: color 0.3s ease;
        }

        /* Hover effect */
        .floating-sidebar a:hover {
            color: #FFD700;
            /* Golden color on hover */
        }
    </style>
</head>

<body>
    <div class="container">
        <h1 style="color: #333;">Quantization in Large Language Models (LLMs)</h1>

        <div class="section">
            <h2>1. Introduction</h2>
            <p><em>Conversion from a higher memory format to a lower memory format to optimize models for faster
                    inference and reduced resource usage.</em></p>
        </div>

        <div class="section">
            <h2>2. Quantization Techniques</h2>

            <div class="subsection">
                <h3>1. Full Precision / Half Precision</h3>
                <ul>
                    <li>
                        <strong>Full Precision (FP32):</strong>
                        <ul>
                            <li>Standard 32-bit floating-point representation</li>
                            <li>High accuracy but resource-intensive (memory and compute)</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Half Precision (FP16/BF16):</strong>
                        <ul>
                            <li>16-bit floating-point format</li>
                            <li>Reduces memory and computation by half while maintaining reasonable accuracy</li>
                            <li><strong>BF16</strong> (bfloat16) keeps a wider exponent range than FP16, making it more
                                robust for training large models</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <div class="subsection">
                <h3>2. Calibration</h3>
                <ul>
                    <li>The process of mapping floating-point values to lower-precision representations (e.g., INT8)
                    </li>
                    <li>Involves analyzing the dynamic range of activations and weights using representative data</li>
                    <li>Helps determine <strong>scale</strong> and <strong>zero-point</strong> for quantization,
                        ensuring minimal loss in model accuracy</li>
                    <li>Essential in <strong>Post-Training Quantization</strong> to maintain performance</li>
                </ul>
            </div>

            <div class="subsection">
                <h3>3. Modes of Quantization</h3>

                <div class="sub-subsection">
                    <h4>a. Post-Training Quantization (PTQ)</h4>
                    <ul>
                        <li>Applied <strong>after</strong> model training</li>
                        <li>Converts model weights and/or activations to lower precision (e.g., FP32 ‚Üí INT8)</li>
                        <li>Quick and easy but may lead to slight accuracy drops</li>
                        <li><strong>Calibration</strong> is often used here to optimize performance</li>
                    </ul>
                </div>

                <div class="sub-subsection">
                    <h4>b. Quantization Aware Training (QAT)</h4>
                    <ul>
                        <li>Simulates quantization during training</li>
                        <li>The model learns to adjust its parameters to minimize quantization-induced errors</li>
                        <li>Leads to better accuracy retention compared to PTQ</li>
                        <li>Heavily used when high accuracy is critical post-quantization</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <br>

    <div class="container">
        <div class="quantization-guide">
            <!-- Number Storage Section -->
            <div class="number-storage">
                <h2>How Numbers Are Stored</h2>
                <div class="container">
                    <!-- FP32 Format -->
                    <div class="format-section fp32">
                        <h3>FP32 (IEEE 754 Standard)</h3>

                        <div class="bit-layout">
                            <div class="bit-segment sign">Sign (1 bit)</div>
                            <div class="bit-segment exponent">Exponent (8 bits)</div>
                            <div class="bit-segment mantissa">Mantissa (23 bits)</div>
                        </div>

                        <div class="components">
                            <h4>Components Explained</h4>
                            <ul>
                                <li><strong>Sign Bit (S):</strong> 0 = Positive, 1 = Negative</li>
                                <li><strong>Exponent (E):</strong> Stored with bias=127 (Actual exponent = E - 127)</li>
                                <li><strong>Mantissa (M):</strong> Implicit leading 1 (1.M format)</li>
                            </ul>
                        </div>

                        <div class="calculation">
                            <h4>FP32 Value Formula</h4>
                            <div class="formula">
                                Value = (-1)<sup>S</sup> √ó 1.M √ó 2<sup>(E - 127)</sup>
                            </div>
                        </div>

                        <div class="example">
                            <h4>Example: -6.75 in FP32</h4>
                            <table class="representation-table">
                                <tr>
                                    <th>Component</th>
                                    <th>Value</th>
                                    <th>Bits</th>
                                </tr>
                                <tr>
                                    <td>Sign (S)</td>
                                    <td>1</td>
                                    <td>1</td>
                                </tr>
                                <tr>
                                    <td>Exponent (E)</td>
                                    <td>129</td>
                                    <td>10000001</td>
                                </tr>
                                <tr>
                                    <td>Mantissa (M)</td>
                                    <td>10110000000000000000000</td>
                                    <td>23 bits</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <!-- FP16 Format -->
                    <div class="format-section fp16">
                        <h3>FP16 (Half Precision)</h3>

                        <div class="bit-layout">
                            <div class="bit-segment sign">Sign (1 bit)</div>
                            <div class="bit-segment exponent">Exponent (5 bits)</div>
                            <div class="bit-segment mantissa">Mantissa (10 bits)</div>
                        </div>

                        <div class="specs">
                            <h4>Key Specifications</h4>
                            <table class="spec-table">
                                <tr>
                                    <th>Feature</th>
                                    <th>Value</th>
                                </tr>
                                <tr>
                                    <td>Exponent Range</td>
                                    <td>-14 to +15</td>
                                </tr>
                                <tr>
                                    <td>Largest Positive Value</td>
                                    <td>~65504</td>
                                </tr>
                                <tr>
                                    <td>Precision</td>
                                    <td>~3 decimal digits</td>
                                </tr>
                            </table>
                        </div>
                    </div>
                

                <!-- Special Values Section -->
                <div class="special-values">
                    <h3>Special Values</h3>
                    <table class="special-values-table">
                        <tr>
                            <th>Exponent</th>
                            <th>Mantissa</th>
                            <th>Representation</th>
                        </tr>
                        <tr>
                            <td>All 0s</td>
                            <td>All 0s</td>
                            <td>Zero</td>
                        </tr>
                        <tr>
                            <td>All 0s</td>
                            <td>Non-zero</td>
                            <td>Denormalized</td>
                        </tr>
                        <tr>
                            <td>All 1s</td>
                            <td>All 0s</td>
                            <td>Infinity</td>
                        </tr>
                        <tr>
                            <td>All 1s</td>
                            <td>Non-zero</td>
                            <td>NaN</td>
                        </tr>
                    </table>
                </div>
            </div>
        </div>
        <br>
        <div class="container">
            <div class="fp16-storage">
                <h3>How Numbers Are Stored in FP16 (Half Precision - IEEE 754 Standard)</h3>

                <div class="description">
                    <p><strong>FP16</strong> (16-bit floating-point) is a lower-precision format than
                        <strong>FP32</strong>,
                        using only 16 bits to represent a real number. It is widely used in deep learning for faster
                        computations and reduced memory usage, especially during training and inference.</p>
                </div>

                <div class="bit-layout">
                    <h4>1. FP16 Bit Layout:</h4>
                    <div class="bit-box">
                        <span class="sign-bit">Sign (1 bit)</span> |
                        <span class="exponent-bits">Exponent (5 bits)</span> |
                        <span class="mantissa-bits">Mantissa (10 bits)</span>
                    </div>
                    <ul class="bit-details">
                        <li><strong>Total Bits:</strong> 16</li>
                        <li><strong>Sign Bit (S):</strong> 1 bit</li>
                        <li><strong>Exponent (E):</strong> 5 bits (Bias = 15)</li>
                        <li><strong>Mantissa (M):</strong> 10 bits</li>
                    </ul>
                </div>

                <div class="formula-section">
                    <h4>2. FP16 Formula:</h4>
                    <div class="formula">
                        Value = (-1)<sup>S</sup> √ó 1.M √ó 2<sup>(E - 15)</sup>
                    </div>
                    <ul class="variables">
                        <li><em>S</em> ‚Üí Sign bit</li>
                        <li><em>E</em> ‚Üí Exponent (with a bias of <strong>15</strong>)</li>
                        <li><em>M</em> ‚Üí Mantissa (with an implicit leading <strong>1</strong> for normalized numbers)
                        </li>
                    </ul>
                </div>

                <div class="range-precision">
                    <h4>3. FP16 Range and Precision:</h4>
                    <table class="spec-table">
                        <tr>
                            <th>Aspect</th>
                            <th>FP16</th>
                        </tr>
                        <tr>
                            <td>Exponent Range</td>
                            <td>-14 to +15</td>
                        </tr>
                        <tr>
                            <td>Smallest Positive</td>
                            <td>‚âà6.1√ó10<sup>-5</sup></td>
                        </tr>
                        <tr>
                            <td>Largest Positive</td>
                            <td>‚âà65504</td>
                        </tr>
                        <tr>
                            <td>Precision</td>
                            <td>‚âà3 decimal digits</td>
                        </tr>
                    </table>
                    <p class="note">FP16 is less precise than FP32 and prone to underflow/overflow but offers
                        significant
                        speed and memory benefits.</p>
                </div>

                <div class="example-section">
                    <h4>4. Example: Representing -6.75 in FP16</h4>
                    <div class="step">
                        <h4>Step 1: Convert to Binary</h4>
                        <p>6.75 in decimal ‚Üí 110.11 in binary</p>
                    </div>

                    <div class="step">
                        <h4>Step 2: Normalize</h4>
                        <div class="formula">
                            110.11 = 1.1011 √ó 2<sup>2</sup>
                        </div>
                    </div>

                    <div class="step">
                        <h4>Step 3: Assemble FP16 Representation</h4>
                        <table class="representation-table">
                            <tr>
                                <th>Component</th>
                                <th>Value</th>
                                <th>Bits</th>
                            </tr>
                            <tr>
                                <td>Sign (S)</td>
                                <td>1</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>Exponent (E)</td>
                                <td>17</td>
                                <td>10001</td>
                            </tr>
                            <tr>
                                <td>Mantissa (M)</td>
                                <td>1011000000</td>
                                <td>10 bits</td>
                            </tr>
                        </table>
                    </div>

                    <div class="final-binary">
                        <p>Final FP16 binary:</p>
                        <code>1 | 10001 | 1011000000</code>
                    </div>
                </div>

                <div class="special-values">
                    <h4>5. Special Values in FP16:</h4>
                    <table class="special-table">
                        <tr>
                            <th>Exponent (E)</th>
                            <th>Mantissa (M)</th>
                            <th>Representation</th>
                        </tr>
                        <tr>
                            <td><code>00000</code></td>
                            <td><code>0000000000</code></td>
                            <td>0 (¬±0)</td>
                        </tr>
                        <tr>
                            <td><code>00000</code></td>
                            <td>Non-zero</td>
                            <td>Denormalized numbers</td>
                        </tr>
                        <tr>
                            <td><code>11111</code></td>
                            <td><code>0000000000</code></td>
                            <td>Infinity (¬±‚àû)</td>
                        </tr>
                        <tr>
                            <td><code>11111</code></td>
                            <td>Non-zero</td>
                            <td>NaN (Not a Number)</td>
                        </tr>
                    </table>
                </div>

                <div class="use-cases">
                    <h4>6. Use Cases in Deep Learning:</h4>
                    <ul class="case-list">
                        <li><strong>Mixed Precision Training:</strong> FP16 with FP32 gradients for faster training</li>
                        <li><strong>Reduced Memory Footprint:</strong> Enables larger batch sizes</li>
                        <li><strong>Tensor Cores:</strong> Optimized for NVIDIA GPUs (Volta+)</li>
                    </ul>
                </div>
            </div>
        </div><br>
        <div class="container">
            <div class="quantization-methods">
                <div class="quantization-methods">
                    <h2>How to Perform Quantization?</h2>

                    <div class="methods-container">
                        <div class="method symmetric">
                            <h3>1. Symmetric Quantization</h3>
                            <ul>
                                <li>Uses equal positive/negative ranges</li>
                                <li>Zero-point fixed at 0</li>
                                <li>Formula:
                                    <div class="formula">q = round(r/s)</div>
                                </li>
                            </ul>
                        </div>

                        <div class="method-asymmetric">
                            <h3>2. Asymmetric Quantization</h3>
                            <ul>
                                <li>Uses flexible zero-point</li>
                                <li>Formula:
                                    <div class="formula">q = round(r/s) + z</div>
                                </li>
                                <li>Better for non-zero-centered data</li>
                            </ul>
                        </div>
                    </div>

                    <div class="batch-normalization-note">
                        <div class="connection"></div>
                        <div class="batch-norm-label">(Batch Normalization)</div>
                        <p>Symmetric unsigned int8 quantization</p>
                    </div>
                </div>
                <h2>How to Perform Quantization?</h2>


                <p>Quantization maps high-precision floating-point numbers (e.g., FP32) to lower-precision integers
                    (e.g.,
                    INT8) to optimize memory usage and computational efficiency. Two primary methods are
                    <strong>Symmetric</strong> and <strong>Asymmetric</strong> quantization.
                </p>

                <div class="method symmetric">
                    <h3>1. Symmetric Quantization</h3>

                    <div class="formula">
                        <code>q = round(r/s)</code>
                        <div class="variables">
                            <p>Where:</p>
                            <ul>
                                <li><em>q</em> = Quantized integer value</li>
                                <li><em>r</em> = Original floating-point value</li>
                                <li><em>s</em> = Scale factor:
                                    <code>s = max(|r<sub>min</sub>|, |r<sub>max</sub>|) / Q<sub>max</sub></code>
                                </li>
                                <li><em>Q<sub>max</sub></em> = Maximum quantized value (e.g., 127 for INT8)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="key-points">
                        <h4>Key Characteristics:</h4>
                        <ul>
                            <li>No zero-point offset</li>
                            <li>Ideal for zero-centered data (weights)</li>
                            <li>Faster computation</li>
                            <li>Simpler implementation</li>
                        </ul>
                    </div>

                    <div class="example">
                        <h4>Example Calculation:</h4>
                        <p>Input range: [‚àí5.0, 5.0]</p>
                        <p>Scale factor: <code>s = 5.0 / 127 ‚âà 0.0394</code></p>
                        <p>Quantize <em>r</em> = ‚àí3.2:</p>
                        <code>q = round(‚àí3.2 / 0.0394) = ‚àí81</code>
                    </div>
                </div>

                <div class="method asymmetric">
                    <h3>2. Asymmetric Quantization</h3>

                    <div class="formula">
                        <code>q = round(r/s) + z</code>
                        <div class="variables">
                            <p>Where:</p>
                            <ul>
                                <li><em>z</em> = Zero-point:
                                    <code>z = round(‚àír<sub>min</sub>/s)</code>
                                </li>
                                <li><em>s</em> = Scale factor:
                                    <code>s = (r<sub>max</sub> ‚àí r<sub>min</sub>) / (Q<sub>max</sub> ‚àí Q<sub>min</sub>)</code>
                                </li>
                                <li>Quantized range: [Q<sub>min</sub>, Q<sub>max</sub>] (e.g., [0, 255] for UINT8)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="key-points">
                        <h4>Key Characteristics:</h4>
                        <ul>
                            <li>Includes zero-point adjustment</li>
                            <li>Handles non-symmetric distributions (activations)</li>
                            <li>Better for ReLU outputs</li>
                            <li>More computationally intensive</li>
                        </ul>
                    </div>

                    <div class="example">
                        <h4>Example Calculation:</h4>
                        <p>Input range: [2.0, 10.0] ‚Üí UINT8 [0, 255]</p>
                        <p>Scale factor: <code>s = (10.0 ‚àí 2.0)/255 ‚âà 0.0314</code></p>
                        <p>Zero-point: <code>z = round(‚àí2.0/0.0314) = ‚àí64 ‚Üí clipped to 0</code></p>
                        <p>Quantize <em>r</em> = 5.0:</p>
                        <code>q = round(5.0/0.0314) + 0 = 159</code>
                    </div>
                </div>

                <div class="comparison">
                    <h3>3. Symmetric vs. Asymmetric Quantization</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>Symmetric</th>
                                <th>Asymmetric</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Zero-Point</td>
                                <td>Always 0</td>
                                <td>Non-zero</td>
                            </tr>
                            <tr>
                                <td>Data Alignment</td>
                                <td>Zero-centered</td>
                                <td>Arbitrary range</td>
                            </tr>
                            <tr>
                                <td>Typical Use</td>
                                <td>Weights</td>
                                <td>Activations</td>
                            </tr>
                            <tr>
                                <td>Complexity</td>
                                <td>Low</td>
                                <td>Medium</td>
                            </tr>
                            <tr>
                                <td>Accuracy</td>
                                <td>Requires centered data</td>
                                <td>Handles distributions</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="quantization-guide">
            <!-- PTQ Section -->
            <div class="ptq-section">
                <h2>üöÄ Post-Training Quantization (PTQ) ‚Äî Deep Dive</h2>

                <div class="workflow">
                    <div class="boxed-flow">
                        [Pre-Trained Model] ‚Üí [Calibration] ‚Üí [Quantized Model] ‚Üí [Use Cases]
                    </div>
                </div>

                <!-- Pre-Trained Model -->
                <div class="stage">
                    <h3>1Ô∏è‚É£ Pre-Trained Model üéØ</h3>
                    <p><strong>What Is It?</strong><br>
                        A fully trained model using FP32 precision.</p>

                    <div class="key-points">
                        <h4>Key Points:</h4>
                        <ul>
                            <li>Standard training workflows</li>
                            <li>Supports CV/NLP/any domain</li>
                            <li>Maximum pre-quantization accuracy</li>
                        </ul>
                    </div>
                </div>

                <!-- Calibration -->
                <div class="stage">
                    <h3>2Ô∏è‚É£ Calibration ‚öñÔ∏è</h3>
                    <div class="process">
                        <h4>Process:</h4>
                        <ol>
                            <li>Select representative dataset (100-1000 samples)</li>
                            <li>Run forward pass without weight updates</li>
                            <li>Calculate scale (s) and zero-point (z)</li>
                        </ol>
                    </div>

                    <div class="calibration-methods">
                        <h4>Calibration Methods</h4>
                        <table>
                            <tr>
                                <th>Method</th>
                                <th>Description</th>
                                <th>Use Case</th>
                            </tr>
                            <tr>
                                <td>Min-Max Scaling</td>
                                <td>Direct mapping of min/max values</td>
                                <td>Simple models</td>
                            </tr>
                            <tr>
                                <td>Percentile Clipping</td>
                                <td>Ignores extreme outliers</td>
                                <td>Data with outliers</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <!-- Quantized Model -->
                <div class="stage">
                    <h3>3Ô∏è‚É£ Quantized Model ‚ö°</h3>
                    <div class="schemes">
                        <div class="symmetric">
                            <h4>Symmetric Quantization</h4>
                            <code>q = round(r/s)</code>
                            <p>Zero-point = 0 (weights)</p>
                        </div>
                        <div class="asymmetric">
                            <h4>Asymmetric Quantization</h4>
                            <code>q = round(r/s) + z</code>
                            <p>Non-zero zero-point (activations)</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- QAT Section -->
            <div class="qat-section">
                <h2>üöÄ Quantization Aware Training (QAT)</h2>

                <div class="workflow">
                    <div class="boxed-flow">
                        [Pre-Trained Model] ‚Üí [Fake Quantization] ‚Üí [Fine-Tuning] ‚Üí [Quantized Model]
                    </div>
                </div>

                <div class="components">
                    <h3>Key Components</h3>
                    <table>
                        <tr>
                            <th>Component</th>
                            <th>Description</th>
                        </tr>
                        <tr>
                            <td>Fake Quantization</td>
                            <td>Simulates quantization during forward pass</td>
                        </tr>
                        <tr>
                            <td>Learnable Weights</td>
                            <td>Adapts to quantization noise</td>
                        </tr>
                        <tr>
                            <td>Gradient Handling</td>
                            <td>Uses Straight-Through Estimator (STE) to backpropagate through quantized nodes</td>
                        </tr>
                        <tr>
                            <td>Fine-Tuning</td>
                            <td>Typically done on a pre-trained model with lower learning rates</td>
                        </tr>
                    </table>
                    <h3>Advantages of QAT Over PTQ:</h3>
                    <ul>
                      <li><strong>Higher Accuracy:</strong> Especially for sensitive models (e.g., Transformers, Object Detectors)</li>
                      <li><strong>Robustness to Quantization Noise:</strong> Model learns to compensate during training</li>
                      <li><strong>Better for Dynamic Ranges:</strong> Handles outliers and activation shifts more effectively</li>
                    </ul>
                  
                    <h3>QAT Workflow in Practice:</h3>
                    <ol>
                      <li><strong>Start with Pre-trained FP32 Model</strong></li>
                      <li><strong>Insert Fake Quantization Nodes</strong> (framework-specific)</li>
                      <li><strong>Fine-tune Model</strong> (small learning rate)</li>
                      <li><strong>Convert to INT8</strong> for efficient deployment</li>
                    </ol>
                </div>
            </div>

        </div>
    </div>
</body>

</html>