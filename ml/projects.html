<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- MathJax for Equations -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

    <style>
        /* Global Styles */
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 0;
            color: #ffffff;
            background: url('../assets/images/ml-Background.jpg') no-repeat center center fixed;
            background-size: cover;
            position: relative;
        }

        /* Dark Overlay */
        body::before {
            content: "";
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: -1;
        }

        /* Layout Containers */
        header,
        footer {
            background: linear-gradient(135deg, #0829df, #04013c);
            text-align: center;
            padding: 20px;
            font-size: 24px;
            position: relative;
            z-index: 10;
        }

        main {
            max-width: 900px;
            margin: 40px auto;
            padding: 25px;
            background: rgba(19, 50, 185, 0.85);
            border-radius: 10px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 10;
        }

        h1,
        h2,
        h3 {
            color: #d9dcdf;
        }

        /* Styling for Key Sections */
        .case h3 {
            color: #ffcc00;
        }

        .case p {
            color: #ffffff;
        }

        /* Table Styles */
        table {
            font-family: 'Courier New', monospace;
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
            background: rgba(255, 255, 255, 0.1);
        }

        th,
        td {
            border: 1px solid #ccc;
            padding: 8px;
            text-align: center;
            font-size: 14px;
        }

        th {
            background: rgba(0, 0, 0, 0.2);
        }

        /* Image Styling */
        .image-container {
            text-align: center;
            margin: 10px 0;
        }

        .image-container img {
            width: 35%;
            border-radius: 5px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
        }

        /* Content Alignment */
        .content-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 10px;
        }

        .image-container {
            flex: 1;
        }

        .image-container img {
            width: 65%;
            border-radius: 10px;
        }

        .text-container {
            flex: 1;
            width: 55%;
        }

        /* Failure Cases Section */
        .failure-cases {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            text-align: justify;
        }

        .case {
            width: 100%;
            background: rgba(255, 255, 255, 0.15);
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 6px;
        }

        /* Decision Tree Node Styles */
        .node circle {
            fill: #69b3a2;
            stroke: #555;
            stroke-width: 2px;
        }

        .node text {
            font-size: 14px;
            font-family: 'Courier New', monospace;
            fill: white;
        }

        .link {
            fill: none;
            stroke: #ccc;
            stroke-width: 2px;
        }

        * {
            user-select: text;
        }
    </style>
</head>

<header>
    <h1>Projects Worked On </h1>
</header>

<body>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

            <div>
                <h2>Introduction</h2>
                <div>
                    <p>AI has the power to transform industries, and I’ve dedicated my career to building intelligent
                        solutions that drive efficiency and decision-making. My name is <strong>Ankit Dutta</strong>,
                        with 9 years of professional experience, including 6 years specializing in AI/ML, NLP, and
                        data-driven solutions across healthcare, telecom, and energy forecasting.</p>

                    <h3>Current Role</h3>
                    <p>As a <strong>Senior Software Engineer (AI/ML)</strong> at Carelon Global Solutions, I develop
                        AI-driven healthcare solutions, including:</p>
                    <ul>
                        <li>✅ LLM-based AI frameworks like an Agentic AI Bot using LlamaIndex.</li>
                        <li>✅ Fine-tuned Google ELECTRA for intent classification and RoBERTa-based medical entity
                            extraction.</li>
                        <li>✅ Medical document classification model (Conv1D-LSTM) achieving 92% accuracy and 80% manual
                            review reduction.</li>
                    </ul>

                    <h3>Previous Experience</h3>
                    <ul>
                        <li>🔹 <strong>EnigmaSoft Technologies (ML Developer)</strong>: Built NLP Email Tagger and
                            Python Selenium scrapers for telecom pricing analysis.</li>
                        <li>🔹 <strong>Ramtech/Reliance Jio</strong>: Developed time-series forecasting models (Seasonal
                            ARIMA, XGBoost) for energy demand and QoS prediction.</li>
                    </ul>

                    <h3>Technical Expertise</h3>
                    <ul>
                        <li><strong>Programming:</strong> Python (TensorFlow, PyTorch, Pandas)</li>
                        <li><strong>NLP/LLMs:</strong> ELECTRA, RoBERTa, Attention models</li>
                        <li><strong>Tools:</strong> Selenium, LlamaIndex, BeautifulSoup</li>
                    </ul>

                    <p>Passionate about solving healthcare challenges through AI/ML innovation and automation. Focused
                        on turning data into actionable insights that improve outcomes.</p>
                </div>
            </div>
        </div>
    </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="project-pitch">
                <h2>Agentic Bot Framework for Healthcare Account Plan Analysis</h2>
                <p>"One of my most impactful projects was developing an Agentic Bot Framework that streamlined
                    healthcare account plan data analysis using LlamaIndex. The goal was to create an intelligent system
                    comprised of specialized agents to analyze Elevance Health data through structured workflows,
                    enabling comprehensive healthcare data analysis and natural language query processing."</p>

                <h3>Problem Statement:</h3>
                <p>Healthcare account plans often contain vast amounts of fragmented data—ranging from benefit names to
                    cost shares across different network types. Navigating this data, validating its distribution, and
                    extracting meaningful insights manually is not only time-consuming but also prone to errors.</p>

                <p><strong>Our mission?</strong> Build an intelligent, agent-driven framework that simplifies plan
                    information access, enables real-time data validation, and even integrates operational tools like
                    Jira for task management.</p>

                <h3>Architecture Overview:</h3>

                <h4>Data Ingestion & Storage:</h4>
                <ul>
                    <li>Since the FOBS data was stored in MongoDB with each benefit name as a key, there was no need for
                        data chunking.</li>
                    <li>Using OpenAI embeddings, we processed the benefit name data directly, enabling semantic
                        understanding and allowing the system to retrieve data contextually rather than relying on
                        simple keyword matching.</li>
                    <li>The processed embeddings were stored in Postgres as a Vector DB, providing fast and accurate
                        similarity searches for user queries.</li>
                </ul>

                <h4>Agentic Bot – User Interaction:</h4>
                <p>The Agentic Bot served as the primary interface for users, offering several powerful tools:</p>
                <ul>
                    <li><strong>Tool 1:</strong> Get Plan Info – Users could input any benefit name and instantly
                        retrieve detailed plan information.</li>
                    <li><strong>Tool 2:</strong> Get Statistics – This tool allowed users to generate statistical
                        summaries for specific groups, plans, or benefits, helping them make data-driven decisions.</li>
                    <li><strong>Tool 3:</strong> Engage Advisor via Jira – Using a Java API for Jira, users could create
                        tickets, track issue statuses, or escalate tasks directly from the bot.</li>
                </ul>

                <h4>Seamless Jira Integration:</h4>
                <p>The Java API for Jira was crucial in connecting our system with project management workflows. It
                    allowed the bot to not only log issues but also monitor ticket progress, ensuring smooth
                    communication between data teams and operational staff.</p>

                <h3>Workflows:</h3>
                <ul>
                    <p>User is assigned with a session ID to track interactions with the bot.</p>
                    <li><strong>User Query:</strong> Any information requested related to a benefit name.</li>
                    <li><strong>Bot Internal:</strong> Analyzes the query and extracts entities like benefit name, group
                        ID, plan name, and contract code.
                        <ul>
                            <li>If group ID is missing:
                                <ul>
                                    <li><strong>Bot Response:</strong> Requests the user to provide the group ID.</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li><strong>User Query:</strong> Provides group ID.</li>
                    <li><strong>Bot Internal:</strong> Validates the group ID, checks its association with the account,
                        and stores it in Redis for session management.
                        <ul>
                            <li>Once all mandatory metadata is available, the bot queries the database to retrieve
                                benefit details.</li>
                            <li>The top 5 results are sent to the LLM along with the user’s question.</li>
                        </ul>
                    </li>
                    <li><strong>Bot Response:</strong> The LLM generates an answer based on the top 5 results.</li>
                    <li><strong>User Query:</strong> If the user requests statistics for a group.</li>
                    <li><strong>Bot Internal:</strong> Executes stored procedures using the metadata, retrieves the
                        data, and sends it to the LLM to generate statistics in a specified format.</li>
                </ul>

                <h3>How It All Comes Together:</h3>
                <p>"Picture a healthcare analyst needing to verify cost-sharing details for a specific benefit. With the
                    Agentic Bot, they simply input the benefit name, and within seconds, the bot fetches validated plan
                    information, complete with timelines and cost breakdowns. If discrepancies arise, the analyst can
                    create a Jira ticket directly through the bot, streamlining issue tracking and resolution—all
                    without switching between multiple systems."</p>

                <h3>Key Technologies & Methodologies:</h3>
                <ul>
                    <li>LlamaIndex for structuring and querying complex healthcare data.</li>
                    <li>OpenAI Embeddings for semantic understanding and data processing.</li>
                    <li>Oracle, MongoDB & Postgres Vector DB for efficient data storage and retrieval.</li>
                    <li>Java API for seamless Jira integration.</li>
                    <li>Robust validation pipelines ensuring data integrity.</li>
                </ul>

                <h3>Impact & Achievements:</h3>
                <ul>
                    <li>60% reduction in data retrieval and validation time.</li>
                    <li>Improved data accuracy through multi-layered validation.</li>
                    <li>Enhanced user experience with intuitive tools, leading to faster decision-making.</li>
                    <li>Streamlined operations by integrating Jira, reducing communication gaps between analysts and
                        technical teams.</li>
                </ul>

                <h3>Final Thought:</h3>
                <p>"This project showcased my ability to blend AI, data engineering, and workflow automation to solve
                    real-world problems. It wasn’t just about building a bot; it was about creating a system that
                    empowered users to interact with complex healthcare data effortlessly while ensuring operational
                    efficiency."</p>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case">
            <div>
                <h2>1. High-Level Overview</h2>
                <p>
                    My part was to implement an intent classification model using a pretrained ELECTRA model in
                    TensorFlow.
                    It processes a dataset of text inputs labeled with different intents (e.g., 'update', 'delete'),
                    fine-tunes the ELECTRA model, and evaluates its performance. The trained model can then be used to
                    predict the intent of new text inputs.
                </p>

                <h2>2. Key Components Breakdown</h2>

                <h3>A. Data Processing</h3>
                <p>
                    We start by defining intent labels and loading a dataset from a JSON file.
                    The dataset is split into training and testing sets. We then use the ELECTRA tokenizer to preprocess
                    text by converting words into tokenized representations, padding them to a fixed length,
                    and mapping labels to numerical IDs.
                </p>
                <ul>
                    <li>✅ Preprocessing function: Tokenizes text and assigns numerical labels</li>
                    <li>✅ Filtering invalid labels: Removes any data points with missing labels</li>
                </ul>

                <h3>B. Model Selection & Training</h3>
                <p>
                    We use the TFElectraForSequenceClassification model, which is a transformer-based model designed for
                    classification tasks. If a previously trained model exists, we load it; otherwise, we use a
                    pre-trained
                    ELECTRA model. We fine-tune it using Adam optimizer and Sparse Categorical Crossentropy loss.
                </p>
                <ul>
                    <li>✅ Model Loading: Tries to load a saved model; otherwise, initializes a pretrained ELECTRA model
                    </li>
                    <li>✅ Compilation: Uses Adam optimizer and cross-entropy loss</li>
                    <li>✅ Training: Runs for 15 epochs with validation on the test dataset</li>
                </ul>

                <h3>C. Model Evaluation</h3>
                <p>
                    Once trained, we evaluate the model on the test set by computing classification metrics like
                    precision,
                    recall, and F1-score using sklearn's classification report. Predictions are generated, and we
                    compare
                    them against ground-truth labels.
                </p>
                <ul>
                    <li>✅ Predictions: Extracted from model logits</li>
                    <li>✅ Metrics: Uses classification_report to assess model performance</li>
                </ul>

                <h3>D. Making Predictions</h3>
                <p>
                    We define a <code>predict_intent</code> function that takes raw text, tokenizes it, passes it
                    through
                    the trained model, and maps predicted labels back to human-readable intents. This allows us to
                    classify
                    new text inputs.
                </p>
                <ul>
                    <li>✅ Tokenizes new inputs</li>
                    <li>✅ Generates predictions</li>
                    <li>✅ Maps back to intent labels</li>
                </ul>

                <h2>3. Why This Approach?</h2>
                <p>
                    Using ELECTRA for intent classification is efficient because it is a lightweight, pre-trained model
                    designed for NLP tasks. It performs well with relatively small amounts of labeled data, making it a
                    great choice for text classification tasks in real-world applications.
                </p>


                <h2>5. Handling Follow-up Questions</h2>

                <h3>Q: Why did you choose ELECTRA over BERT?</h3>
                <p>
                    ELECTRA is more parameter-efficient than BERT. Instead of just predicting masked words like BERT, it
                    learns from replaced tokens, making it more sample-efficient and faster to train.
                </p>

                <h3>Q: How would you improve the model’s performance?</h3>
                <p>
                    I’d experiment with larger models like electra-base or electra-large, fine-tune using more
                    domain-specific
                    data, and apply data augmentation techniques.
                </p>

                <h3>Q: How would you deploy this model?</h3>
                <p>
                    I’d save the model using <code>save_pretrained()</code>, package it in a Docker container, and
                    deploy it as
                    an API using FastAPI or AWS Lambda.
                </p>

            </div>

        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <div>
                    <h2>1. High-Level Overview</h2>
                    <p>
                        This script implements a Named Entity Recognition (NER) model using a pretrained RoBERTa model
                        in TensorFlow.
                        It processes a dataset of text inputs with labeled entities (e.g., "benefitName", "uom"),
                        fine-tunes the RoBERTa model,
                        and evaluates its performance. The trained model can then extract entities from new text inputs.
                    </p>

                    <h2>2. Key Components Breakdown</h2>

                    <h3>A. Data Processing</h3>
                    <p>
                        We define entity labels and load a dataset from a JSON file. The dataset is split into training
                        and testing sets.
                        We use the RoBERTa tokenizer to preprocess text by converting words into tokenized
                        representations and mapping
                        entity labels to token-level annotations.
                    </p>
                    <ul>
                        <li>✅ Entity Labels: Defined for various entity types like "benefitName", "uom", and
                            "benefitOption"</li>
                        <li>✅ Tokenization: Converts words into tokenized sequences while preserving entity alignment
                        </li>
                        <li>✅ Label Mapping: Assigns entity labels to tokenized words</li>
                    </ul>

                    <h3>B. Model Selection & Training</h3>
                    <p>
                        We use the <code>TFRobertaForTokenClassification</code> model, which is a transformer-based
                        model designed for
                        token-level classification. If a previously trained model exists, we load it; otherwise, we use
                        a pre-trained
                        RoBERTa model and fine-tune it using an Adam optimizer.
                    </p>
                    <ul>
                        <li>✅ Model Loading: Loads a saved model if available; otherwise, initializes a pretrained
                            RoBERTa model</li>
                        <li>✅ Compilation: Uses Adam optimizer and cross-entropy loss</li>
                        <li>✅ Training: Runs for multiple epochs with validation on the test dataset</li>
                    </ul>

                    <h3>C. Model Evaluation</h3>
                    <p>
                        Once trained, we evaluate the model on the test set by computing classification metrics like
                        precision, recall,
                        and F1-score using sklearn’s classification report. Predictions are generated and compared
                        against ground-truth labels.
                    </p>
                    <ul>
                        <li>✅ Predictions: Extracted from model logits</li>
                        <li>✅ Metrics: Uses classification_report to assess model performance</li>
                    </ul>

                    <h3>D. Making Predictions</h3>
                    <p>
                        We define a <code>predict_entities</code> function that takes raw text, tokenizes it, passes it
                        through the trained model,
                        and maps predicted entity labels back to meaningful outputs. This allows us to extract
                        structured information from text.
                    </p>
                    <ul>
                        <li>✅ Tokenizes new inputs</li>
                        <li>✅ Generates entity predictions</li>
                        <li>✅ Maps tokens back to entity labels</li>
                    </ul>

                    <h2>3. Why This Approach?</h2>
                    <p>
                        Using RoBERTa for NER is effective because it captures contextual meaning in text, allowing it
                        to generalize well to
                        unseen data. The model is optimized for token-level classification and performs well with
                        relatively small amounts
                        of labeled data.
                    </p>

                    <h2>4. Possible Extensions</h2>
                    <ul>
                        <li>✅ Fine-tune on a larger dataset for better accuracy</li>
                        <li>✅ Optimize hyperparameters (learning rate, batch size)</li>
                        <li>✅ Deploy the trained model using AWS Lambda or FastAPI for real-time entity extraction</li>
                    </ul>

                    <h2>5. Handling Follow-up Questions</h2>

                    <h3>Q: Why did you choose RoBERTa over BERT?</h3>
                    <p>
                        RoBERTa is an optimized version of BERT that removes next-sentence prediction and uses dynamic
                        masking,
                        making it more robust and better suited for token classification tasks like NER.
                    </p>

                    <h3>Q: How would you improve the model’s performance?</h3>
                    <p>
                        I’d experiment with larger models like roberta-large, fine-tune using more domain-specific data,
                        and apply
                        data augmentation techniques.
                    </p>

                    <h3>Q: How would you deploy this model?</h3>
                    <p>
                        I’d save the model using <code>save_pretrained()</code>, package it in a Docker container, and
                        deploy it as
                        an API using FastAPI or AWS Lambda.
                    </p>

                    <h2>Final Tip</h2>
                    <p>
                        When explaining, stay structured, be concise, and adapt to the interviewer’s focus. Emphasize
                        your
                        understanding of model training, evaluation, and deployment—not just the code itself.
                    </p>
                </div>

            </div>

        </div>
    </div>




    </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Electricity Demand Forecasting - Project Pitch</h2>

                <h3>🔹 Introduction</h3>
                <p>Accurate electricity demand forecasting is critical for utilities and grid operators. It reduces
                    investment risks, optimizes fuel purchases, and ensures reliable power supply by balancing
                    generation and consumption.</p>
                <p>This project focused on <strong>medium-term forecasting</strong>, specifically predicting electricity
                    demand <strong>one week in advance</strong> at an hourly resolution for London. Historical data from
                    2002-2020 was used, leveraging time series analysis and regression models to generate forecasts.</p>

                <h3>🔹 Data Collection</h3>
                <ul>
                    <li><strong>Electricity Demand Data:</strong> Provided by County of London Electric Supply Company,
                        cleaned by handling missing values and duplicates.</li>
                    <li><strong>Weather Data:</strong> Sourced from Weather Underground, capturing temperature,
                        humidity, wind speed, and weather conditions, which were preprocessed for consistency.</li>
                </ul>

                <h3>🔹 Feature Engineering</h3>
                <ul>
                    <li>Weather conditions converted into 6 broad categories (clear, cloud, rain, tstorm, fog, snow)
                        using one-hot encoding.</li>
                    <li>Created <strong>temporal features</strong> (weekday flag, sine/cosine encoding for time-of-day
                        effects).</li>
                    <li>Normalized continuous features to improve model performance.</li>
                </ul>

                <h3>🔹 Model Design</h3>
                <p>The forecasting problem was broken down into two parts:</p>
                <ul>
                    <li><strong>Monthly Trends (SARIMA):</strong> Captures long-term seasonal trends in demand.</li>
                    <li><strong>Hourly Residuals (Regression):</strong> Predicts the difference between actual demand
                        and monthly trend using machine learning models.</li>
                </ul>

                <h3>🔹 Models Used</h3>
                <ul>
                    <li><strong>SARIMA:</strong> Modeled monthly electricity demand trends with clear seasonality and
                        population growth.</li>
                    <li><strong>Linear Regression:</strong> Provided a baseline but lacked predictive power.</li>
                    <li><strong>Gradient Boosting Regression (GBR):</strong> Best performer, improving Mean Absolute
                        Percentage Error (MAPE) by 61%.</li>
                    <li><strong>Multi-Layer Perceptron (MLP):</strong> Outperformed linear regression but underperformed
                        GBR due to hyperparameter tuning challenges.</li>
                </ul>

                <h3>🔹 Key Takeaways</h3>
                <ul>
                    <li>Best approach: <strong>SARIMA + Gradient Boosting Regression (GBR)</strong> for optimal
                        accuracy.</li>
                    <li>Future work: Improved hyperparameter tuning, adding contextual features (e.g., holidays,
                        events).</li>
                    <li>GBR model demonstrated reliable <strong>one-week-out electricity demand forecasts</strong>,
                        making it a viable solution for power grid management.</li>
                </ul>

                <h3>💡 Why This Matters?</h3>
                <p>By combining traditional time series modeling with machine learning, we achieved more accurate
                    electricity demand predictions. This approach helps utilities optimize resource allocation, reduce
                    costs, and improve energy efficiency.</p>

                <h3>🚀 Final Pitch</h3>
                <p>“Our project successfully developed a hybrid electricity demand forecasting model, leveraging SARIMA
                    for long-term trends and Gradient Boosting for short-term predictions. The model achieved a 61%
                    improvement over the baseline, offering a practical solution for utilities to enhance energy
                    planning and grid stability.”</p>
            </div>


        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="failure-cases">
                <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        
                    <div>
                        <h2>1. High-Level Overview</h2>
                        <p>"Retrieval-Augmented Generation (RAG) is an AI technique that enhances large language models (LLMs)
                            by retrieving external knowledge before generating responses. Instead of relying solely on a model's
                            internal memory, RAG dynamically searches a knowledge base, extracts relevant information, and
                            integrates it into the generation process. This improves accuracy, reduces hallucinations, and
                            enables responses based on the latest available information."</p>
        
                        <h2>2. Key Components Breakdown</h2>
        
                        <h3>A. Data Preprocessing: Chunking & Vectorization</h3>
                        <p>🔹 Before retrieval can happen, raw text data (e.g., documents, PDFs, or web pages) must be processed
                            into smaller, searchable units called chunks.</p>
        
                        <ul>
                            <li><strong>Chunking:</strong> "Long documents are broken into smaller, meaningful sections (e.g.,
                                paragraphs or fixed-size text segments). This ensures that retrieval returns the most relevant
                                context rather than entire documents."</li>
                            <li><strong>Vectorization:</strong> "Each chunk is converted into an embedding (a numerical
                                representation) using an embedding model (e.g., OpenAI’s text-embedding models, BERT, or SBERT).
                                These embeddings capture semantic meaning and are stored in a vector database for efficient
                                similarity search."</li>
                        </ul>
        
                        <p><strong>💡 Why Chunking & Vectorization?</strong></p>
                        <ul>
                            <li>Prevents retrieving excessive or irrelevant text</li>
                            <li>Improves the efficiency of search queries</li>
                            <li>Helps the model focus on specific, relevant pieces of information</li>
                        </ul>
        
                        <h3>B. Retrieval Phase</h3>
                        <p>"When a user asks a question, their query is also converted into an embedding using the same
                            embedding model. This embedding is then compared against stored document embeddings in a vector
                            database (e.g., FAISS, Pinecone, ChromaDB) to find the most relevant chunks."</p>
        
                        <ul>
                            <li>✅ Uses similarity search to find relevant text chunks</li>
                            <li>✅ Supports keyword-based (BM25) and dense vector search</li>
                            <li>✅ Retrieves only the most relevant knowledge for augmentation</li>
                        </ul>
        
                        <h3>C. Augmentation & Fusion Phase</h3>
                        <p>"The retrieved chunks are combined with the original query before being fed into a language model.
                            This step ensures that the model generates responses grounded in factual, retrieved knowledge."</p>
        
                        <ul>
                            <li>✅ Reduces hallucinations by forcing the model to use real data</li>
                            <li>✅ Fusion strategies include simple concatenation or attention-based integration</li>
                            <li>✅ Helps the model answer knowledge-intensive queries more accurately</li>
                        </ul>
        
                        <h3>D. Generation Phase</h3>
                        <p>"Finally, the augmented query is processed by a generative model (e.g., GPT-4, LLaMA, or T5), which
                            uses both its pre-trained knowledge and the retrieved chunks to generate a final response."</p>
        
                        <ul>
                            <li>✅ Uses retrieved information + model's internal knowledge</li>
                            <li>✅ Ensures responses are relevant, fact-based, and context-aware</li>
                            <li>✅ Improves interpretability and trustworthiness of AI-generated answers</li>
                        </ul>
        
                        <h2>3. Why Use RAG?</h2>
                        <p>"Traditional LLMs are static—they cannot update their knowledge without expensive fine-tuning. RAG
                            solves this by dynamically fetching external knowledge, allowing models to stay up to date without
                            retraining."</p>
        
                        <p><strong>💡 Key Benefits:</strong></p>
                        <ul>
                            <li>✅ More accurate responses with real-world knowledge</li>
                            <li>✅ Reduces hallucinations (misleading or false information)</li>
                            <li>✅ Lower computational cost compared to full fine-tuning</li>
                            <li>✅ Can be easily updated by refreshing the knowledge base</li>
                        </ul>
        
                        <h2>4. Possible Extensions & Optimizations</h2>
                        <ul>
                            <li>✅ <strong>Hybrid Search:</strong> Combining keyword search (BM25) with dense vector search for
                                better retrieval</li>
                            <li>✅ <strong>Multi-Document Fusion:</strong> Retrieving multiple relevant chunks and merging
                                insights</li>
                            <li>✅ <strong>Memory-Augmented RAG:</strong> Keeping track of previous conversations for contextual
                                continuity</li>
                            <li>✅ <strong>RAG + Agents:</strong> Using agentic workflows where retrieved knowledge helps in
                                multi-step reasoning</li>
                        </ul>
        
                        <h2>5. Handling Follow-up Questions</h2>
        
                        <h3>❓ Q: How does RAG compare to fine-tuning?</h3>
                        <p>"Fine-tuning updates a model’s internal parameters, making it expensive and requiring periodic
                            retraining. RAG, on the other hand, retrieves external knowledge dynamically, allowing real-time
                            updates without modifying the model itself."</p>
        
                        <h3>❓ Q: What challenges exist in RAG implementation?</h3>
                        <p>"Challenges include retrieval latency, noisy data affecting response quality, and maintaining an
                            efficient knowledge base. These can be mitigated with optimized chunking strategies, ranking
                            algorithms, and indexing improvements."</p>
        
                        <h3>❓ Q: How would you deploy RAG in production?</h3>
                        <p>"I’d use a vector database like FAISS or Pinecone to store embeddings, integrate an API-based LLM for
                            generation, and optimize retrieval latency using caching and ranking techniques."</p>
        
                        <h2>Final Tip</h2>
                        <p>"When explaining RAG, focus on its <strong>practical advantages, real-world applications, and how it
                                enhances LLM capabilities.</strong> Stay structured, provide clear examples, and be ready to
                            discuss trade-offs!" 🚀</p>
                    </div>
        
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>




</body>

</html>