<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Mechanism Example</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- MathJax for Equations -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    <style>
        /* Global Styles */
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 0;
            color: #ffffff;
            background: url('../assets/images/ml-Background.webp') no-repeat center center fixed;
            background-size: cover;
            position: relative;
        }

        /* Dark Overlay */
        body::before {
            content: "";
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: -1;
        }

        /* Layout Containers */
        header,
        footer {
            background: linear-gradient(135deg, #0829df, #04013c);
            text-align: center;
            padding: 20px;
            font-size: 24px;
            position: relative;
            z-index: 10;
        }

        main {
            max-width: 900px;
            margin: 40px auto;
            padding: 25px;
            background: rgba(19, 50, 185, 0.85);
            border-radius: 10px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 10;
        }

        h1,
        h2,
        h3 {
            color: #d9dcdf;
        }

        /* Styling for Key Sections */
        .case h3 {
            color: #ffcc00;
        }

        .case p {
            color: #ffffff;
        }

        /* Table Styles */
        table {
            font-family: 'Courier New', monospace;
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
            background: rgba(255, 255, 255, 0.1);
        }

        th,
        td {
            border: 1px solid #ccc;
            padding: 8px;
            text-align: center;
            font-size: 14px;
        }

        th {
            background: rgba(0, 0, 0, 0.2);
        }

        /* Image Styling */
        .image-container {
            text-align: center;
            margin: 10px 0;
        }

        .image-container img {
            width: 35%;
            border-radius: 5px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
        }

        /* Content Alignment */
        .content-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 10px;
        }

        .image-container {
            flex: 1;
        }

        .image-container img {
            width: 65%;
            border-radius: 10px;
        }

        .text-container {
            flex: 1;
            width: 55%;
        }

        /* Failure Cases Section */
        .failure-cases {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            text-align: justify;
        }
        .attention-mechanism {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            text-align: justify;
        }
        .case {
            width: 100%;
            background: rgba(255, 255, 255, 0.15);
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 6px;
        }

        /* Decision Tree Node Styles */
        .node circle {
            fill: #69b3a2;
            stroke: #555;
            stroke-width: 2px;
        }

        .node text {
            font-size: 14px;
            font-family: 'Courier New', monospace;
            fill: white;
        }

        .link {
            fill: none;
            stroke: #ccc;
            stroke-width: 2px;
        }

        a {
            color: #ffcc00;
            text-decoration: underline;
            font-weight: bold;
        }

        a:hover {
            color: #ffffff;
            text-decoration: none;
        }

        .floating-sidebar {
            position: fixed;
            top: 50%;
            left: -120px;
            /* Partially hidden */
            width: 150px;
            /* Explicit width */
            transform: translateY(-50%);
            background-color: rgba(143, 137, 137, 0.5);
            padding: 15px;
            border-radius: 0 10px 10px 0;
            box-shadow: 2px 2px 10px rgb(255, 255, 255);
            transition: left 0.3s ease;
            /* Smooth slide */
        }

        /* Reveal sidebar on hover */
        .floating-sidebar:hover {
            left: 0;
            /* Fully visible */
        }

        /* Sidebar Link */
        .floating-sidebar a {
            color: white;
            text-decoration: none;
            font-size: 18px;
            font-family: Arial, sans-serif;
            display: block;
            padding: 10px 0;
            transition: color 0.3s ease;
        }

        /* Hover effect for links */
        .floating-sidebar a:hover {
            color: #FFD700;
            /* Golden color on hover */
        }
    </style>

</head>

<body>
    <div class="failure-cases">
        <h1>Attention Mechanism Example</h1>

        <h2>Vocabulary</h2>
        <table>
            <tr>
                <th>Word</th>
                <th>Index</th>
            </tr>
            <tr>
                <td>the</td>
                <td>0</td>
            </tr>
            <tr>
                <td>cat</td>
                <td>1</td>
            </tr>
            <tr>
                <td>sat</td>
                <td>2</td>
            </tr>
            <tr>
                <td>on</td>
                <td>3</td>
            </tr>
            <tr>
                <td>wall</td>
                <td>4</td>
            </tr>
        </table>

        <h2>Sample Sentence</h2>
        <p><strong>the cat sat</strong></p>

        <h2>Embedding (X)</h2>
        <pre>
            [[1,0,0,0], 
             [0,1,0,0], 
             [0,0,1,0]]
        </pre>

        <h2>Initialize (d=2) - Wq, Wk, Wv</h2>
        <pre>
            Wq = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
            Wk = [[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]
            Wv = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
        </pre>

        <h2>Calculate Q, K, V</h2>
        <pre>
            Q = X * Wq = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
            K = X * Wk = [[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]
            V = X * Wv = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
        </pre>

        <h2>Attention Scores</h2>
        <pre>
            Q * K^T = [[0.09, 0.12, 0.15], 
                       [0.19, 0.26, 0.33], 
                       [0.29, 0.4 , 0.51]]
        </pre>

        <h2>Normalize by sqrt(d)</h2>
        <pre>
            [[0.0636, 0.0848, 0.1060],
             [0.1343, 0.1838, 0.2333],
             [0.2050, 0.2828, 0.3606]]
        </pre>

        <h2>Softmax of Attention Scores</h2>
        <pre>
            [[0.3262, 0.3332, 0.3404],
             [0.3169, 0.3330, 0.3499],
             [0.3077, 0.3326, 0.3595]]
        </pre>

        <h2>Compute Output</h2>
        <pre>
            Output = Attention Scores * V
            [[0.3028, 0.4028],
             [0.3065, 0.4065],
             [0.3103, 0.4103]]
        </pre>

        <h2>Compute Probabilities</h2>
        <pre>
            W out = [[0.2, 0.3, 0.1, 0.2, 0.3, 0.4],  
                     [0.4, 0.5, 0.5, 0.6, 0.6, 0.7]]
            b out = [0.1, 0.2, 0.3, 0.1, 0.0, 0.2]
    
            Logits = (Output ⋅ W out) + b out
            Logits1 = [0.3216, 0.4922, 0.5316, 0.4022, 0.3325, 0.6031]
        </pre>

        <h2>Softmax(Logits1, Logits2, Logits3)</h2>
        <pre>
            P = [[0.6, 0.1, 0.1, 0.2, 0], 
                 [0, 0.6, 0.2, 0.1, 0.1], 
                 [0, 0, 0.7, 0.1, 0.2]]
        </pre>

        <h2>Compute Loss</h2>
        <pre>
            Loss1 = -log(0.6)
            Loss2 = -log(0.6)
            Loss3 = -log(0.7)
            Loss = (1/3) * (Loss1 + Loss2 + Loss3)
        </pre>

        <h2>Compute Gradients</h2>
        <pre>
            ∂P/∂Loss = P - One-Hot(Target)
            
            ∂Loss/∂P1 = [ 0.6, 0.1, 0.1, 0.2, 0] - [1, 0, 0, 0, 0]  
                       = [-0.4, 0.1, 0.1, 0.2, 0]
    
            ∂Loss/∂P2 = [0, 0.6, 0.2, 0.1, 0.1] - [0, 1, 0, 0, 0]  
                       = [0, -0.4, 0.2, 0.1, 0.1]
    
            ∂Loss/∂P3 = [0, 0, 0.7, 0.1, 0.2] - [0, 0, 1, 0, 0]  
                       = [0, 0, -0.3, 0.1, 0.2]
        </pre>


        <h3>Compute Gradient of Loss w.r.t. Attention Weights</h3>
        <pre>
        ∂Loss/∂Attention_Scores = ∂Loss/∂Output ⋅ V^T
        
        ∂Loss/∂V = Attention_Scores^T ⋅ ∂Loss/∂Output
        
        ∂Loss/∂Wq = X^T ⋅ ∂Loss/∂Q
        ∂Loss/∂Wk = X^T ⋅ ∂Loss/∂K
        ∂Loss/∂Wv = X^T ⋅ ∂Loss/∂V
        </pre>

    <div>
        <h3>Compute Numerical Gradients</h3>
        <pre>
        ∂Loss/∂Attention_Scores = [[-0.02, 0.01, 0.03],  
                                    [0.04, -0.03, 0.02],  
                                    [0.01, 0.02, -0.01]]
        
        ∂Loss/∂V = [[0.012, -0.018],  
                     [0.023, 0.009],  
                     [-0.014, 0.027]]
        </pre>
    </div>

    <p>This completes the gradient computation for attention weights.</p>

    </div>

</body>

<body>
    <div class="attention-mechanism">
        <h1>Attention Mechanism Example</h1>

        <h2>Vocabulary</h2>
        <table>
            <tr>
                <th>Word</th>
                <th>Index</th>
            </tr>
            <tr>
                <td>the</td>
                <td>0</td>
            </tr>
            <tr>
                <td>cat</td>
                <td>1</td>
            </tr>
            <tr>
                <td>sat</td>
                <td>2</td>
            </tr>
            <tr>
                <td>on</td>
                <td>3</td>
            </tr>
            <tr>
                <td>wall</td>
                <td>4</td>
            </tr>
        </table>

        <h2>Sample Sentence</h2>
        <p><strong>the cat sat</strong></p>

        <h2>Embedding (X)</h2>
        <pre>
[[1,0,0,0], 
 [0,1,0,0], 
 [0,0,1,0]]
        </pre>

        <h2>Initialize (d=2) - Wq, Wk, Wv</h2>
        <pre>
Wq = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
Wk = [[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]
Wv = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
        </pre>

        <h2>Calculate Q, K, V</h2>
        <pre>
Q = X * Wq = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
K = X * Wk = [[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]
V = X * Wv = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
        </pre>

        <h2>Attention Scores</h2>
        <pre>
Q * K^T = [[0.09, 0.12, 0.15], 
           [0.19, 0.26, 0.33], 
           [0.29, 0.4 , 0.51]]
        </pre>

        <h2>Normalize by sqrt(d)</h2>
        <pre>
[[0.0636, 0.0848, 0.1060],
 [0.1343, 0.1838, 0.2333],
 [0.2050, 0.2828, 0.3606]]
        </pre>

        <h2>Softmax of Attention Scores</h2>
        <pre>
[[0.3262, 0.3332, 0.3404],
 [0.3169, 0.3330, 0.3499],
 [0.3077, 0.3326, 0.3595]]
        </pre>

        <h2>Compute Output</h2>
        <pre>
Output = Attention Scores * V
[[0.3028, 0.4028],
 [0.3065, 0.4065],
 [0.3103, 0.4103]]
        </pre>

        <h2>Compute Probabilities</h2>
        <pre>
W out = [[0.2, 0.3, 0.1, 0.2, 0.3, 0.4],  
         [0.4, 0.5, 0.5, 0.6, 0.6, 0.7]]
b out = [0.1, 0.2, 0.3, 0.1, 0.0, 0.2]

Logits = (Output ⋅ W out) + b out
Logits1 = [0.3216, 0.4922, 0.5316, 0.4022, 0.3325, 0.6031]
        </pre>

        <h2>Softmax(Logits1, Logits2, Logits3)</h2>
        <pre>
P = [[0.6, 0.1, 0.1, 0.2, 0], 
     [0, 0.6, 0.2, 0.1, 0.1], 
     [0, 0, 0.7, 0.1, 0.2]]
        </pre>

        <h2>Compute Loss</h2>
        <pre>
Loss1 = -log(0.6)
Loss2 = -log(0.6)
Loss3 = -log(0.7)
Loss = (1/3) * (Loss1 + Loss2 + Loss3)
        </pre>

        <h2>Compute Gradients</h2>
        <pre>
∂P/∂Loss = P - One-Hot(Target)

∂Loss/∂P1 = [-0.4, 0.1, 0.1, 0.2, 0]
∂Loss/∂P2 = [0, -0.4, 0.2, 0.1, 0.1]
∂Loss/∂P3 = [0, 0, -0.3, 0.1, 0.2]
        </pre>

        <h2>Compute Gradient of Loss w.r.t. Attention Weights</h2>
        <pre>
∂Loss/∂Attention_Scores = ∂Loss/∂Output ⋅ V^T
∂Loss/∂V = Attention_Scores^T ⋅ ∂Loss/∂Output
∂Loss/∂Wq = X^T ⋅ ∂Loss/∂Q
∂Loss/∂Wk = X^T ⋅ ∂Loss/∂K
∂Loss/∂Wv = X^T ⋅ ∂Loss/∂V
        </pre>

        <h2>Compute Numerical Gradients</h2>
        <pre>
∂Loss/∂Attention_Scores = [[-0.02, 0.01, 0.03],  
                            [0.04, -0.03, 0.02],  
                            [0.01, 0.02, -0.01]]

∂Loss/∂V = [[0.012, -0.018],  
             [0.023, 0.009],  
             [-0.014, 0.027]]
        </pre>

        <p>This completes the gradient computation for attention weights.</p>

    </div>
</body>

</html>